"""
Trained Bidirectional LSTM Model - Test Set Prediction
Load saved model parameters, predict on the test set and evaluate
"""
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import re
import time
import os
import sys
import json
from pathlib import Path
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import KFold, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score
from datetime import datetime, timedelta
warnings.filterwarnings('ignore')

# Set Chinese font display
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


def save_attention_visual(attn_weights, save_dir, prefix):
    """
    Save matrix plot and time step heatmap of attention weights.
    attn_weights: torch.Tensor, shape could be [B, heads, T, T] or [B, T, T]
    """
    if attn_weights is None:
        return
    os.makedirs(save_dir, exist_ok=True)
    attn = attn_weights.detach().cpu()
     # Unify to [B, T, T]
    if attn.dim() == 4:
        attn = attn.mean(dim=1)
    attn_np = attn.numpy()
    if attn_np.shape[0] == 0:
        return
    mat = attn_np[0] # Take the first sample
    # Matrix plot
    plt.figure(figsize=(4, 3))
    im = plt.imshow(mat, cmap='Blues', aspect='auto')
    plt.colorbar(im)
    plt.title(f'Attention Matrix ({prefix})')
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, f'attention_matrix_{prefix}.png'), dpi=300)
    plt.close()
    # Time step weights (average by source or target)
    heat = mat.mean(axis=0)
    plt.figure(figsize=(4, 3))
    plt.plot(heat, marker='o', linewidth=1.2)
    plt.title(f'Attention Weight per Time Step ({prefix})')
    plt.xlabel('Time Step')
    plt.ylabel('Weight')
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, f'attention_heat_{prefix}.png'), dpi=300)
    plt.close()

# Set random seeds and device
torch.manual_seed(42)
np.random.seed(42)

# Check GPU availability
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"使用设备: {device}")
if torch.cuda.is_available():
    print(f"GPU型号: {torch.cuda.get_device_name(0)}")
    print(f"GPU内存: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
else:
    print("未检测到GPU，将使用CPU计算")

# ========== Import necessary classes and functions ==========
# Ensure project root directory is accessible
CURRENT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = CURRENT_DIR.parent  # Pi_BiLSTM root directory
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))
REPO_ROOT = PROJECT_ROOT.parent  # constitutive_relation
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from dataset.dataloader import (
    load_excel_data as load_dataset_with_clusters,
    ConstitutiveDataset as ClusterAwareDataset
)
from Bidirectional_LSTM_Enhanced交叉验证 import (
    BidirectionalLSTMRegressor,
    EnhancedCurveLoss, calculate_curve_metrics, calculate_dtw_distance,
    plot_test_predictions
)


def load_excel_data(material_params_file, stress_data_file, curve_length, train_indices=None):
    """
     Wrapper function for backward compatibility, reusing implementation from dataset.dataloader.。
    """
    result = load_dataset_with_clusters(
        material_params_file=material_params_file,
        stress_data_file=stress_data_file,
        curve_length=curve_length,
        train_indices=train_indices,
        cache_dir=None,
        use_cache=True,
        default_cluster_count=None,
        verbose=True
    )
    return result[:16]

# ========== Model loading and prediction functions ==========
def load_trained_model(model_path):
    """
    Load the trained model
    
    Args:
        model_path: Path to the model file
    
    Returns:
        model: Loaded model
        model_info: Dictionary of model information
    """
    print(f"Loading model: {model_path}")
    
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model file does not exist: {model_path}")
    
    # Load model checkpoint (PyTorch 2.6+ default weights_only=True may cause deserialization failure)
    # Model files generated by this project are from trusted sources, explicitly disable weights_only for compatibility with old formats
    checkpoint = torch.load(model_path, map_location=device, weights_only=False)
    
    # Extract model information
    model_info = {
        'model_params': checkpoint['model_params'],
        'best_params': checkpoint['best_params'],
        'material_param_names': checkpoint['material_param_names'],
        'curve_length': checkpoint['curve_length'],
        'strain_scaler': checkpoint['strain_scaler'],
        'stress_scaler': checkpoint['stress_scaler'],
        'material_scaler': checkpoint['material_scaler'],
        'peak_stress_scaler': checkpoint['peak_stress_scaler'],
        'peak_strain_scaler': checkpoint['peak_strain_scaler']
    }
    
   # Optional fields
    if 'epoch' in checkpoint:
        model_info['epoch'] = checkpoint['epoch']
    if 'loss' in checkpoint:
        model_info['loss'] = checkpoint['loss']
    if 'best_fold_info' in checkpoint:
        model_info['best_fold_info'] = checkpoint['best_fold_info']
    
    # Create model
    model = BidirectionalLSTMRegressor(
        input_size=len(model_info['material_param_names']) + 3,  # +1 for strain +1 for peak_stress +1 for peak_strain
        output_length=model_info['curve_length'],
        **model_info['model_params']
    )
    
   # Load model weights
    try:
        model.load_state_dict(checkpoint['model_state_dict'], strict=True)
    except RuntimeError as e:
         # If strict loading fails, check for key name mismatch
        checkpoint_keys = set(checkpoint['model_state_dict'].keys())
        model_keys = set(model.state_dict().keys())
        missing_keys = model_keys - checkpoint_keys
        unexpected_keys = checkpoint_keys - model_keys
        
        if 'fc1' in checkpoint_keys and 'fc_layers' in model_keys:
            print("Warning: Detected model checkpoint uses 'fc1' but current model uses 'fc_layers'.")
            print("This may be due to model structure updates. Attempting to load in non-strict mode...")
            # Try non-strict mode loading (ignore mismatched keys)
            model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        else:
            raise RuntimeError(f"Failed to load model weights: {e}\nMissing keys: {missing_keys}\nUnexpected keys: {unexpected_keys}")
    
    model = model.to(device)
    model.eval()
    
    print(f"Model loaded successfully")
    if 'epoch' in model_info:
        print(f"Training epochs: {model_info['epoch']}")
    if 'loss' in model_info:
        print(f"Final loss: {model_info['loss']:.4f}")
    if 'best_fold_info' in model_info:
        print(f"Best fold information: {model_info['best_fold_info']}")
    print(f"Number of material parameters: {len(model_info['material_param_names'])}")
    print(f"Curve length: {model_info['curve_length']}")
    
    # Check model weights
    print(f"\n========== Model weight check ==========")
    total_params = 0
    non_zero_params = 0
    for name, param in model.named_parameters():
        if param.requires_grad:
            total_params += param.numel()
            non_zero_params += (param != 0).sum().item()
            if 'fc_output' in name or 'relu_output' in name:
                print(f"  {name}: shape={param.shape}, 范围=[{param.min().item():.6f}, {param.max().item():.6f}], 均值={param.mean().item():.6f}")
    print(f"Total parameter count: {total_params}")
    print(f"Non-zero parameter count: {non_zero_params}")
    print(f"Non-zero parameter ratio: {non_zero_params/total_params*100:.2f}%")
    if non_zero_params / total_params < 0.9:
        print("⚠️ Warning: More than 10% of model parameters are zero, the model may not be trained correctly!")
    
    # Validate model with test input
    print(f"\n========== Model forward propagation test ==========")
    model.eval()
    with torch.no_grad():
        # Create simple test input
        test_batch_size = 2
        test_curve_length = model_info['curve_length']
        test_num_params = len(model_info['material_param_names'])
        
        # Create reasonable test input (normalized values)
        test_strain = torch.linspace(0.0, 2.0, test_curve_length).unsqueeze(0).repeat(test_batch_size, 1).to(device)
        test_material = torch.ones(test_batch_size, test_curve_length, test_num_params).to(device) * 0.5  # 归一化后[0,1]，取中间值
        test_peak_stress = torch.ones(test_batch_size, test_curve_length).to(device) * 1.0  # 归一化后峰值应力=1
        test_peak_strain = torch.ones(test_batch_size, test_curve_length).to(device) * 1.0  # 归一化后峰值应变=1
        
        test_x = torch.cat([
            test_strain.unsqueeze(-1), 
            test_material, 
            test_peak_stress.unsqueeze(-1), 
            test_peak_strain.unsqueeze(-1)
        ], dim=-1)
        
        test_output = model(test_x)
        print(f"Test input shape: {test_x.shape}")
        print(f"Test output shape: {test_output.shape}")
        print(f"Test output range: [{test_output.min().item():.6f}, {test_output.max().item():.6f}]")
        print(f"Test output mean: {test_output.mean().item():.6f}")
        print(f"Test output standard deviation: {test_output.std().item():.6f}")
        
        if test_output.max().item() < 0.01:
            print("⚠️ Critical warning: Model test output is close to zero! The model may not be trained correctly or has architectural issues!")
        elif test_output.max().item() < 0.1:
            print("⚠️ Warning: Model test output is small, there may be issues")
    print("==========================================\n")
    
    return model, model_info


def _normalize_surrogate_column(values, scaler, key_name):
    """Normalize surrogate column using peak average"""
    if scaler is None or scaler.get('type') != 'peak_average':
         raise ValueError(f"Cannot normalize based on {key_name}, scaler information is missing or unsupported type: {scaler}")
    return values / scaler['factor']


def calculate_xiao_curve_with_real_peaks(sigma_cp, epsilon_cp, r_value, strain_sequence):
    """
    Calculate stress-strain curve according to Xiao et al.'s formula (original scale).
    """
    if sigma_cp is None or not np.isfinite(sigma_cp):
        return np.zeros_like(strain_sequence, dtype=float)
    if epsilon_cp is None or not np.isfinite(epsilon_cp) or epsilon_cp <= 0:
        epsilon_cp = max(float(np.max(strain_sequence)), 1e-4)

    if r_value is None:
        r_value = 0.0
    if r_value > 1:
        r_value = r_value / 100.0

    a = 2.2 * (0.748 * r_value**2 - 1.231 * r_value + 0.975)
    b = 0.8 * (7.6483 * r_value + 1.142)

    strain_sequence = np.asarray(strain_sequence, dtype=float)
    x = np.divide(strain_sequence, epsilon_cp, out=np.zeros_like(strain_sequence, dtype=float), where=epsilon_cp != 0)

    stress_ratio = np.zeros_like(x, dtype=float)
    mask_ascending = x < 1
    if np.any(mask_ascending):
        xa = x[mask_ascending]
        stress_ratio[mask_ascending] = a * xa + (3 - 2 * a) * xa**2 + (a - 2) * xa**3

    mask_descending = ~mask_ascending
    if np.any(mask_descending):
        xd = x[mask_descending]
        denominator = b * (xd - 1) ** 2 + xd
        denominator = np.where(np.abs(denominator) < 1e-8, 1e-8, denominator)
        stress_ratio[mask_descending] = xd / denominator

    stress_curve = stress_ratio * sigma_cp
    return np.nan_to_num(stress_curve, nan=0.0, posinf=sigma_cp, neginf=0.0)


def calculate_yan_curve_with_real_peaks(sigma_cp, epsilon_cp, wa_value, strain_sequence):
    """
    Calculate stress-strain curve according to Yan et al.'s formula (original scale).
    """
    if sigma_cp is None or not np.isfinite(sigma_cp):
        return np.zeros_like(strain_sequence, dtype=float)
    if epsilon_cp is None or not np.isfinite(epsilon_cp) or epsilon_cp <= 0:
        epsilon_cp = max(float(np.max(strain_sequence)), 1e-4)

    if wa_value is None:
        wa_value = 0.0
    if wa_value > 1:
        wa_value = wa_value / 100.0

    a = 0.00795 * wa_value**2 + 0.03273 * wa_value + 1.7762
    b = -0.0264 * wa_value**2 + 0.70578 * wa_value + 0.97629

    strain_sequence = np.asarray(strain_sequence, dtype=float)
    x = np.divide(strain_sequence, epsilon_cp, out=np.zeros_like(strain_sequence, dtype=float), where=epsilon_cp != 0)

    stress_ratio = np.zeros_like(x, dtype=float)
    mask_ascending = x < 1
    if np.any(mask_ascending):
        xa = x[mask_ascending]
        stress_ratio[mask_ascending] = a * xa + (3 - 2 * a) * xa**2 + (a - 2) * xa**3

    mask_descending = ~mask_ascending
    if np.any(mask_descending):
        xd = x[mask_descending]
        denominator = b * (xd - 1) ** 2 + xd
        denominator = np.where(np.abs(denominator) < 1e-8, 1e-8, denominator)
        stress_ratio[mask_descending] = xd / denominator

    stress_curve = stress_ratio * sigma_cp
    return np.nan_to_num(stress_curve, nan=0.0, posinf=sigma_cp, neginf=0.0)


# ========== Energy analysis related functions ==========
def compute_curve_energy(strain: np.ndarray, stress: np.ndarray, 
                         residual_ratio: float = None, peak_stress: float = None) -> float:
     """
    Calculate the area under a single stress-strain curve (energy absorbed per unit volume, MPa ≈ MJ/m³)
    
    Args:
        strain: Strain array
        stress: Stress array
        residual_ratio: Residual strength ratio (e.g., 0.2 for 20%), if provided, only calculate up to the strain corresponding to this residual strength
        peak_stress: Peak stress, required if residual_ratio is not None
    
    Returns:
        Energy value (energy absorbed per unit volume)
    """
    if strain.size == 0 or stress.size == 0:
        return 0.0

     # Ensure integration in increasing strain order to prevent minor curve disorder
    order = np.argsort(strain)
    strain_sorted = strain[order]
    stress_sorted = stress[order]
    
    # If residual strength ratio is specified, only calculate up to the corresponding strain
    if residual_ratio is not None and peak_stress is not None:
        # Find peak position
        peak_idx = np.argmax(stress_sorted)
        peak_stress_actual = stress_sorted[peak_idx]
        peak_strain_actual = strain_sorted[peak_idx]
        
        # Find critical point based on normalized stress-strain curve
        # Normalization: x = ε / ε_cp, y = σ / σ_cp
        # In normalized coordinates, 20% residual strength corresponds to y = 0.2
        residual_stress_normalized = residual_ratio
        
        # Normalize strain and stress
        normalized_strain = strain_sorted / peak_strain_actual if peak_strain_actual > 1e-8 else strain_sorted
        normalized_stress = stress_sorted / peak_stress_actual if peak_stress_actual > 1e-8 else stress_sorted
        
         # Find critical point in descending segment (after peak)
        critical_idx = peak_idx
        if peak_idx < len(normalized_stress) - 1:
           # Find first point in descending segment where normalized stress drops below residual strength
            descending_stress_norm = normalized_stress[peak_idx + 1:]
            descending_indices = np.where(descending_stress_norm <= residual_stress_normalized)[0]
            
            if len(descending_indices) > 0:
                # Found first satisfying point (index relative to peak_idx+1)
                critical_idx = peak_idx + 1 + descending_indices[0]
            else:
                # If no point found in descending segment, curve didn't drop to residual strength after peak, use last point
                critical_idx = len(stress_sorted) - 1
        else:
            # If peak is at the end, can't calculate descending segment, use last point
            critical_idx = len(stress_sorted) - 1
        
         # Ensure critical index is valid (should be at or after peak position)
        critical_idx = min(critical_idx, len(strain_sorted) - 1)
        critical_idx = max(critical_idx, peak_idx)
        
        # Only calculate energy from initial point to critical point
        strain_truncated = strain_sorted[:critical_idx + 1]
        stress_truncated = stress_sorted[:critical_idx + 1]
        return float(np.trapz(stress_truncated, strain_truncated))
    else:
        # Use entire curve
        return float(np.trapz(stress_sorted, strain_sorted))


def compute_energy_indicators(strain: np.ndarray, stress: np.ndarray, 
                              residual_ratio: float = None) -> dict:
    """
    Calculate energy indicators: W_u (total energy), W_ascending (ascending segment energy), W_p (descending segment energy), η (energy toughness ratio)
    
    If residual_ratio is specified, calculate two sets of energy indicators:
    - Energy indicators for entire curve (W_u, W_ascending, W_p, eta)
    - Energy indicators corresponding to 20% residual strength (W_u_20%, W_ascending_20%, W_p_20%, eta_20%)
    
    Args:
        strain: Strain array
        stress: Stress array
        residual_ratio: Residual strength ratio (e.g., 0.2 for 20%), if provided, also calculate energy indicators for 20% residual strength
    
    Returns:
        Dictionary containing energy indicators
    """
    if strain.size == 0 or stress.size == 0:
        result = {'W_u': 0.0, 'W_ascending': 0.0, 'W_p': 0.0, 'eta': 0.0}
        if residual_ratio is not None:
            result.update({
                'W_u_20%': 0.0, 'W_ascending_20%': 0.0, 
                'W_p_20%': 0.0, 'eta_20%': 0.0
            })
        return result
    
   # Find peak position
    peak_idx = np.argmax(stress)
    
    # Ensure integration in increasing strain order
    order = np.argsort(strain)
    strain_sorted = strain[order]
    stress_sorted = stress[order]
    
   # Find peak position again (after sorting)
    peak_stress = np.max(stress_sorted)
    peak_idx_sorted = np.where(stress_sorted == peak_stress)[0]
    if len(peak_idx_sorted) > 0:
        peak_idx_sorted = peak_idx_sorted[0]
    else:
        peak_idx_sorted = len(strain_sorted) // 2
    
    # ==========  Energy indicators for entire curve ==========
    # W_u: Total energy (area under entire curve)
    W_u = float(np.trapz(stress_sorted, strain_sorted))
    
    # W_ascending: Ascending segment energy (area before peak point, including peak point)
    if peak_idx_sorted > 0:
        W_ascending = float(np.trapz(stress_sorted[:peak_idx_sorted+1], strain_sorted[:peak_idx_sorted+1]))
    else:
        W_ascending = 0.0
    
    # W_p: Descending segment energy (area after peak point)
    if peak_idx_sorted < len(strain_sorted) - 1:
        W_p = float(np.trapz(stress_sorted[peak_idx_sorted:], strain_sorted[peak_idx_sorted:]))
    else:
        W_p = 0.0
    
    # η: Energy toughness ratio
    if W_u > 1e-8:
        eta = W_p / W_u
    else:
        eta = 0.0
    
    result = {'W_u': W_u, 'W_ascending': W_ascending, 'W_p': W_p, 'eta': eta}
    
    # ==========Energy indicators for 20% residual strength (if specified) ==========
    if residual_ratio is not None:
       # Find critical point based on normalized stress-strain curve
        # Normalization: x = ε / ε_cp, y = σ / σ_cp
        peak_strain = strain_sorted[peak_idx_sorted]
        
        # Normalize strain and stress
        normalized_strain = strain_sorted / peak_strain if peak_strain > 1e-8 else strain_sorted
        normalized_stress = stress_sorted / peak_stress if peak_stress > 1e-8 else stress_sorted
        
         # In normalized coordinates, 20% residual strength corresponds to y = 0.2
        residual_stress_normalized = residual_ratio
        
        # Find critical point in descending segment (after peak)
        critical_idx = peak_idx_sorted
        if peak_idx_sorted < len(normalized_stress) - 1:
            # Find first point in descending segment where normalized stress drops below residual strength
            descending_stress_norm = normalized_stress[peak_idx_sorted + 1:]
            descending_indices = np.where(descending_stress_norm <= residual_stress_normalized)[0]
            
            if len(descending_indices) > 0:
                # Found first satisfying point (index relative to peak_idx_sorted+1)
                critical_idx = peak_idx_sorted + 1 + descending_indices[0]
            else:
                # If no point found in descending segment, curve didn't drop to residual strength after peak, use last point
                critical_idx = len(stress_sorted) - 1
        else:
            # If peak is at the end, can't calculate descending segment, use last point
            critical_idx = len(stress_sorted) - 1
        
        # Ensure critical index is valid (should be at or after peak position)
        critical_idx = min(critical_idx, len(strain_sorted) - 1)
        critical_idx = max(critical_idx, peak_idx_sorted)
        
        # W_u_20%: Total energy (area from initial point to critical point)
        W_u_20 = float(np.trapz(stress_sorted[:critical_idx + 1], strain_sorted[:critical_idx + 1]))
        
        # W_ascending_20%: Ascending segment energy (area before peak point, including peak point)
        if peak_idx_sorted > 0:
            W_ascending_20 = float(np.trapz(stress_sorted[:peak_idx_sorted+1], strain_sorted[:peak_idx_sorted+1]))
        else:
            W_ascending_20 = 0.0
        
        # W_p_20%: Descending segment energy (area from peak point to critical point)
        if peak_idx_sorted < critical_idx:
            W_p_20 = float(np.trapz(
                stress_sorted[peak_idx_sorted:critical_idx + 1], 
                strain_sorted[peak_idx_sorted:critical_idx + 1]
            ))
        else:
            W_p_20 = 0.0
        
        # η_20%: Energy toughness ratio
        if W_u_20 > 1e-8:
            eta_20 = W_p_20 / W_u_20
        else:
            eta_20 = 0.0
        
        result.update({
            'W_u_20%': W_u_20, 
            'W_ascending_20%': W_ascending_20, 
            'W_p_20%': W_p_20, 
            'eta_20%': eta_20
        })
    
    return result


def compute_energy_metrics(true_energy: np.ndarray, pred_energy: np.ndarray) -> dict:
    """
    Calculate error metrics for predicted energy absorption per unit volume
    
    Args:
        true_energy: Array of true energy values
        pred_energy: Array of predicted energy values
    
    Returns:
        Dictionary containing various energy metrics
    """
    true_energy = np.asarray(true_energy, dtype=float)
    pred_energy = np.asarray(pred_energy, dtype=float)

    finite_mask = np.isfinite(true_energy) & np.isfinite(pred_energy)
    metrics = {
        "Energy_R2": float("nan"),
        "Energy_MSE": float("nan"),
        "Energy_RMSE": float("nan"),
        "Energy_MAE": float("nan"),
        "Energy_MAPE": float("nan"),
        "Energy_Bias": float("nan"),
        "Energy_Mean_Ratio": float("nan"),
    }

    valid_count = np.count_nonzero(finite_mask)
    if valid_count == 0:
        return metrics

    te = true_energy[finite_mask]
    pe = pred_energy[finite_mask]

    metrics["Energy_MAE"] = float(mean_absolute_error(te, pe))
    metrics["Energy_MSE"] = float(mean_squared_error(te, pe))
    metrics["Energy_RMSE"] = float(np.sqrt(metrics["Energy_MSE"]))

    if valid_count >= 2:
        try:
            metrics["Energy_R2"] = float(r2_score(te, pe))
        except ValueError:
            metrics["Energy_R2"] = float("nan")

    metrics["Energy_Bias"] = float(np.mean(pe - te))

    denom_mask = np.abs(te) > 1e-8
    if np.count_nonzero(denom_mask) > 0:
        try:
            metrics["Energy_MAPE"] = float(mean_absolute_percentage_error(te[denom_mask], pe[denom_mask]))
        except ValueError:
            metrics["Energy_MAPE"] = float("nan")
        metrics["Energy_Mean_Ratio"] = float(np.mean(pe[denom_mask] / te[denom_mask]))

    for key, value in list(metrics.items()):
        if isinstance(value, float) and (not np.isfinite(value)):
            metrics[key] = None

    return metrics


def plot_test_predictions_seven_curves(pred_curves, true_curves, fc_curves,
                                       xiao_fc_curves, xiao_xgb_curves,
                                       yan_fc_curves, yan_xgb_curves,
                                       test_material_params, strain_data,
                                       save_dir='SAVE/test_predictions',
                                       num_plots=None, strain_scaler=None,
                                       stress_scaler=None, test_labels=None):
    """
    Plot seven curves: True, fc prediction, surrogate prediction, Xiao(fc), Xiao(XGB), Yan(fc), Yan(XGB).
    """
    if pred_curves is None or xiao_xgb_curves is None or yan_xgb_curves is None:
        print("⚠️ Cannot plot seven curves: missing surrogate data.")
        return

    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    os.makedirs(save_dir, exist_ok=True)

    if num_plots is None:
        num_plots = min(20, len(pred_curves))
    else:
        num_plots = min(min(num_plots, len(pred_curves)), 20)

    rows, cols = 4, 5
    figsize = (18, 12)
    fontsize = 9

    fig, axes = plt.subplots(rows, cols, figsize=figsize)
    if rows == 1:
        axes = axes.reshape(1, -1)

    for i in range(min(num_plots, len(pred_curves))):
        row = i // cols
        col = i % cols
        ax = axes[row, col] if rows > 1 else axes[col]

        strain_curve = strain_data[i]
        if strain_scaler and strain_scaler.get('type') == 'peak_average':
            strain_curve = strain_curve * strain_scaler['factor']

        ax.plot(strain_curve, true_curves[i], 'b-', linewidth=1.2, label='Curve 1 (True)', alpha=0.9)
        ax.plot(strain_curve, fc_curves[i], 'g--', linewidth=1.2, label='Curve 2 (Real Peak)', alpha=0.9)
        ax.plot(strain_curve, pred_curves[i], 'r:', linewidth=1.2, label='Curve 3 (Surrogate)', alpha=0.9)
        ax.plot(strain_curve, xiao_fc_curves[i], 'm-.', linewidth=1.0, label='Curve 4 (Xiao, Real)', alpha=0.9)
        ax.plot(strain_curve, xiao_xgb_curves[i], 'c-.', linewidth=1.0, label='Curve 5 (Xiao, Surrogate)', alpha=0.9)
        ax.plot(strain_curve, yan_fc_curves[i], 'y--', linewidth=1.0, label='Curve 6 (Yan, Real)', alpha=0.9)
        ax.plot(strain_curve, yan_xgb_curves[i], color='orange', linestyle='-', linewidth=1.0,
                label='Curve 7 (Yan, Surrogate)', alpha=0.9)

        mse = np.mean((pred_curves[i] - true_curves[i]) ** 2)
        r2 = r2_score(true_curves[i], pred_curves[i]) if len(true_curves[i]) > 1 else 0.0
        material_params = test_material_params[i]
        # Use test label as title, if none use Sample number
        if test_labels is not None and i < len(test_labels):
            sample_label = str(test_labels[i])
        else:
            sample_label = f"Sample {i+1}"
        title = f"{sample_label}\nMSE: {mse:.3f}, R^2: {r2:.3f}"
        if len(material_params) >= 8:
            r_val = material_params[6] if material_params[6] <= 1 else material_params[6] / 100
            wa_val = material_params[7] if material_params[7] <= 1 else material_params[7] / 100
            title += f"\nr: {r_val:.2f}, WA: {wa_val:.2f}"
        ax.set_title(title, fontsize=fontsize, fontweight='bold')
        if i == 0:
            ax.legend(fontsize=fontsize - 2, loc='upper right')
        ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.5)
        ax.tick_params(labelsize=fontsize - 2)

    for i in range(num_plots, rows * cols):
        row = i // cols
        col = i % cols
        if rows > 1:
            axes[row, col].set_visible(False)
        else:
            axes[col].set_visible(False)

    plt.suptitle('Bidirectional LSTM Test Set Prediction Results (7 Curves)', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'test_predictions_comparison_seven_curves.png'),
                dpi=300, bbox_inches='tight')
    #plt.show()


def _normalize_column_name(name):
    return re.sub(r"[^a-z0-9]+", "", str(name).strip().lower())


def find_matching_column(df, candidates):
    normalized_map = {_normalize_column_name(col): col for col in df.columns}
    for candidate in candidates:
        key = _normalize_column_name(candidate)
        if key in normalized_map:
            return normalized_map[key]
    return None


def _load_surrogate_peak_columns(excel_file, peak_stress_scaler, peak_strain_scaler, surrogate_cols):
    material_df = pd.read_excel(excel_file, sheet_name=0)
    stress_candidates = (
        surrogate_cols[0],
        'XGB_fc', 'xgb_fc', 'xgbfc', '预测fc', 'pinn_fc'
    )
    strain_candidates = (
        surrogate_cols[1],
        'CatBoost_peak_strain', 'catboost_peak_strain', 'CatBoost_strain',
        'peak_strain_pinn_pred_xgbfc', 'peak_strain_pinn_pred_xgb_fc', # Keep old column names as alternatives
        'Xiao_strain', 'xgb_xiao_strain', 'xgbxiaostrain'  # Keep old column names as alternatives
    )

    stress_col = find_matching_column(material_df, stress_candidates)
    strain_col = find_matching_column(material_df, strain_candidates)
    if stress_col is None or strain_col is None:
        raise ValueError(f"Surrogate peak columns not found: {surrogate_cols}")

    stress_original = material_df[stress_col].values.astype(float)
    strain_original = material_df[strain_col].values.astype(float)
    stress_normalized = _normalize_surrogate_column(stress_original, peak_stress_scaler, stress_col)
    strain_normalized = _normalize_surrogate_column(strain_original, peak_strain_scaler, strain_col)

    return {
        'stress_original': stress_original,
        'strain_original': strain_original,
        'stress_normalized': stress_normalized,
        'strain_normalized': strain_normalized,
        'stress_col': stress_col,
        'strain_col': strain_col
    }

def plot_test_predictions_four_curves(true_curves, model_curves, xiao_curves, yan_curves, 
                                      test_material_params, strain_data, 
                                      save_dir='SAVE/test_predictions', mode_name='Real Peak Mode',
                                      num_plots=None, strain_scaler=None, stress_scaler=None, test_labels=None):
    """
    Plot test set prediction comparison - four curves: True, Model Prediction, Xiao's Formula, Yan's Formula
    
    Args:
        true_curves: True stress curves [n_samples, curve_length] (Curve 1)
        model_curves: Model predicted stress curves [n_samples, curve_length] (Curve 2)
        xiao_curves: Stress curves calculated by Xiao's formula [n_samples, curve_length] (Curve 3)
        yan_curves: Stress curves calculated by Yan's formula [n_samples, curve_length] (Curve 4)
        test_material_params: Test set material parameters [n_samples, num_params]
        strain_data: Strain data [n_samples, curve_length]
        save_dir: Save directory
        mode_name: Mode name (for title and filename)
        num_plots: Number of samples to plot, plot all if None
    """
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # Create save directory
    os.makedirs(save_dir, exist_ok=True)
    
   # Fixed 4x5 canvas, maximum 20 samples
    if num_plots is None:
        num_plots = min(20, len(true_curves))
    else:
        num_plots = min(min(num_plots, len(true_curves)), 20)
    
    rows = 4
    cols = 5
    figsize = (18, 12)
    fontsize = 9
    
    fig, axes = plt.subplots(rows, cols, figsize=figsize)
    if rows == 1:
        axes = axes.reshape(1, -1)
    
    for i in range(min(num_plots, len(true_curves))):
        row = i // cols
        col = i % cols
        ax = axes[row, col] if rows > 1 else axes[col]
        
        true_curve = true_curves[i]
        model_curve = model_curves[i]
        xiao_curve = xiao_curves[i]
        yan_curve = yan_curves[i]
        strain_curve = strain_data[i]
        
        # Denormalize strain data to original values
        if strain_scaler and strain_scaler['type'] == 'peak_average':
            strain_curve = strain_curve * strain_scaler['factor']
        
        material_params = test_material_params[i]
        
        # Plot four curves
        ax.plot(strain_curve, true_curve, 'b-', linewidth=1.5, label='Curve 1 (True)', alpha=0.9)
        ax.plot(strain_curve, model_curve, 'g--', linewidth=1.5, label='Curve 2 (Model)', alpha=0.9)
        ax.plot(strain_curve, xiao_curve, 'r:', linewidth=1.5, label='Curve 3 (Xiao)', alpha=0.9)
        ax.plot(strain_curve, yan_curve, 'm-.', linewidth=1.5, label='Curve 4 (Yan)', alpha=0.9)
        
         # Set title and legend
        if test_labels is not None and i < len(test_labels):
            sample_label = str(test_labels[i])
        else:
            sample_label = f"Sample {i+1}"
        
       # Calculate model prediction metrics
        mse_model = np.mean((model_curve - true_curve)**2)
        r2_model = r2_score(true_curve, model_curve) if len(true_curve) > 1 else 0.0
        
        material_info = f"{sample_label}\nModel: MSE={mse_model:.3f}, R²={r2_model:.3f}"
        if len(material_params) >= 8:
            r_val = material_params[6] if material_params[6] <= 1 else material_params[6] / 100
            wa_val = material_params[7] if material_params[7] <= 1 else material_params[7] / 100
            material_info += f"\nr: {r_val:.2f}, WA: {wa_val:.2f}"
        
        ax.set_title(material_info, fontsize=fontsize, fontweight='bold')
        ax.set_xlabel('Strain', fontsize=fontsize-1)
        ax.set_ylabel('Stress', fontsize=fontsize-1)
        
         # Show legend only in first subplot
        if i == 0:
            ax.legend(fontsize=fontsize-2, loc='upper right')
        
        ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.5)
        ax.tick_params(labelsize=fontsize-2)
        
        # Set axis ranges
        ax.set_xlim(strain_curve.min() * 0.95, strain_curve.max() * 1.05)
        ax.set_ylim(min(true_curve.min(), model_curve.min(), xiao_curve.min(), yan_curve.min()) * 0.9, 
                   max(true_curve.max(), model_curve.max(), xiao_curve.max(), yan_curve.max()) * 1.1)
    
    # Hide extra subplots
    for i in range(num_plots, rows * cols):
        row = i // cols
        col = i % cols
        if rows > 1:
            axes[row, col].set_visible(False)
        else:
            axes[col].set_visible(False)
    
    plt.suptitle(f'Bidirectional LSTM Test Set Prediction Results ({mode_name})', fontsize=16, fontweight='bold')
    plt.tight_layout()
    
    # Generate filename (remove special characters)
    safe_mode_name = mode_name.replace(' ', '_').replace('/', '_')
    plt.savefig(os.path.join(save_dir, f'test_predictions_four_curves_{safe_mode_name}.png'), 
                dpi=300, bbox_inches='tight')
    #plt.show()


def plot_test_predictions_three_curves(pred_curves, true_curves, fc_curves, test_material_params, strain_data, 
                                       save_dir='SAVE/test_predictions', num_plots=None, strain_scaler=None, stress_scaler=None, test_labels=None):
     """
    Plot test set prediction comparison - three curves: True, fc/strain_m, Prediction
    
    Args:
        pred_curves: Predicted stress curves [n_samples, curve_length] (Curve 3)
        true_curves: True stress curves [n_samples, curve_length] (Curve 1)
        fc_curves: Stress curves from fc/strain_m [n_samples, curve_length] (Curve 2)
        test_material_params: Test set material parameters [n_samples, num_params]
        strain_data: Strain data [n_samples, curve_length]
        save_dir: Save directory
        num_plots: Number of samples to plot, plot all if None
    """
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
     # Create save directory
    os.makedirs(save_dir, exist_ok=True)
    
    # Fixed 4x5 canvas, maximum 20 samples
    if num_plots is None:
        num_plots = min(20, len(pred_curves))
    else:
        num_plots = min(min(num_plots, len(pred_curves)), 20)
    
    rows = 4
    cols = 5
    figsize = (18, 12)  # 扩展画布以容纳更多子图
    fontsize = 9
    markersize = 6
    
    fig, axes = plt.subplots(rows, cols, figsize=figsize)
    if rows == 1:
        axes = axes.reshape(1, -1)
    
    # Calculate overall metrics
    mse_scores = []
    r2_scores = []
    mae_scores = []
    
    for i in range(min(num_plots, len(pred_curves))):
        row = i // cols
        col = i % cols
        ax = axes[row, col] if rows > 1 else axes[col]
        
        pred_curve = pred_curves[i]
        true_curve = true_curves[i]
        fc_curve = fc_curves[i]
        strain_curve = strain_data[i]
        
        # Denormalize strain data to original values
        if strain_scaler and strain_scaler['type'] == 'peak_average':
            strain_curve = strain_curve * strain_scaler['factor']
        
        material_params = test_material_params[i]
        
        # Calculate metrics for this sample (using denormalized data)
        mse = np.mean((pred_curve - true_curve)**2)
        r2 = r2_score(true_curve, pred_curve)
        mae = np.mean(np.abs(pred_curve - true_curve))
        
        mse_scores.append(mse)
        r2_scores.append(r2)
        mae_scores.append(mae)
        
        # Plot three curves
        ax.plot(strain_curve, true_curve, 'b-', linewidth=1.5, label='Curve 1 (True)', alpha=0.9)
        ax.plot(strain_curve, fc_curve, 'g--', linewidth=1.5, label='Curve 2 (fc/strain_m)', alpha=0.9)
        ax.plot(strain_curve, pred_curve, 'r:', linewidth=1.5, label='Curve 3 (Predicted)', alpha=0.9)
        
        # Set title and legend
        # Use test labels as title, if not available use Sample number
        if test_labels is not None and i < len(test_labels):
            sample_label = str(test_labels[i])
        else:
            sample_label = f"Sample {i+1}"
        material_info = f"{sample_label}\nMSE: {mse:.3f}, R^2: {r2:.3f}"
        if len(material_params) >= 8:
            r_val = material_params[6] if material_params[6] <= 1 else material_params[6] / 100  # r (mass replacement ratio) - index 6
            wa_val = material_params[7] if material_params[7] <= 1 else material_params[7] / 100  # WA (mixed aggregate water absorption) - index 7
            material_info += f"\nr: {r_val:.2f}, WA: {wa_val:.2f}"
        
        ax.set_title(material_info, fontsize=fontsize, fontweight='bold')
        ax.set_xlabel('Strain', fontsize=fontsize-1)
        ax.set_ylabel('Stress', fontsize=fontsize-1)
        
        # Only show legend in first subplot
        if i == 0:
            ax.legend(fontsize=fontsize-2, loc='upper right')
        
        ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.5)
        ax.tick_params(labelsize=fontsize-2)
        
        # Set axis limits to ensure curves are fully displayed
        ax.set_xlim(strain_curve.min() * 0.95, strain_curve.max() * 1.05)
        ax.set_ylim(min(true_curve.min(), fc_curve.min(), pred_curve.min()) * 0.9, 
                   max(true_curve.max(), fc_curve.max(), pred_curve.max()) * 1.1)
    
    # Hide extra subplots
    for i in range(num_plots, rows * cols):
        row = i // cols
        col = i % cols
        if rows > 1:
            axes[row, col].set_visible(False)
        else:
            axes[col].set_visible(False)
    
    plt.suptitle(f'Bidirectional LSTM Test Set Prediction Results (Test Mode: 3 Curves)', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'test_predictions_comparison_three_curves.png'), 
                dpi=300, bbox_inches='tight')
    #plt.show()
    
    # Print overall statistics
    print(f"\n=== Test Set Prediction Statistics (All {num_plots} Samples) ===")
    print(f"Mean MSE: {np.mean(mse_scores):.4f} ± {np.std(mse_scores):.4f}")
    print(f"Mean R^2: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}")
    print(f"Mean MAE: {np.mean(mae_scores):.4f} ± {np.std(mae_scores):.4f}")
    
    return mse_scores, r2_scores, mae_scores

def _legacy_predict_test_set(model, model_info, excel_file, save_dir='SAVE/test_predictions'):
    """
    Predict test set using trained model
    
    Args:
        model: trained model
        model_info: model information
        excel_file: Excel data file path
        save_dir: save directory
    
    Returns:
        test_results: test results dictionary
    """
    print("=== Test Set Prediction ===")
    
    # Create save directory
    os.makedirs(save_dir, exist_ok=True)
    
    # Load data
    print("Loading data...")
    X_strain, X_stress, X_material, X_peak_stress, X_peak_strain, material_param_names, strain_scaler, stress_scaler, material_scaler, peak_stress_scaler, peak_strain_scaler, X_material_original, X_stress_original, X_peak_stress_original, X_peak_strain_original, sample_divisions = load_excel_data(excel_file, excel_file, model_info['curve_length'])
    
    print(f"Loaded {len(X_strain)} samples")
    
    # Read XGB_fc and CatBoost_peak_strain for test mode
    print("Reading XGB_fc and CatBoost_peak_strain for test mode...")
    material_df = pd.read_excel(excel_file, sheet_name=0)
    
    # Use find_matching_column function for flexible column name matching
    stress_candidates = ('XGB_fc', 'xgb_fc', 'xgbfc', 'Predicted fc', 'pinn_fc')
    strain_candidates = ('CatBoost_peak_strain', 'catboost_peak_strain', 'CatBoost_strain',
                        'catboost_strain', 'catboost_peakstrain',
                        'peak_strain_pinn_pred_xgbfc', 'peak_strain_pinn_pred_xgb_fc',  # Keep old column names as backup
                        'Xiao_strain', 'xgb_xiao_strain', 'xgbxiaostrain')  # Keep old column names as backup
    
    xgb_fc_col = find_matching_column(material_df, stress_candidates)
    catboost_strain_col = find_matching_column(material_df, strain_candidates)
    
    if xgb_fc_col is not None and catboost_strain_col is not None:
        X_XGB_fc_original = material_df[xgb_fc_col].values
        X_CatBoost_strain_original = material_df[catboost_strain_col].values
        print(f"Successfully read XGB_fc and CatBoost_peak_strain columns: '{xgb_fc_col}', '{catboost_strain_col}'")
        
        print(f"\n========== XGB_fc and CatBoost_peak_strain Data Check ==========")
        print(f"Original XGB_fc range: {np.min(X_XGB_fc_original):.2f} - {np.max(X_XGB_fc_original):.2f}")
        print(f"Original XGB_fc mean: {np.mean(X_XGB_fc_original):.2f}")
        print(f"Original CatBoost_peak_strain range: {np.min(X_CatBoost_strain_original):.4f} - {np.max(X_CatBoost_strain_original):.4f}")
        print(f"Original CatBoost_peak_strain mean: {np.mean(X_CatBoost_strain_original):.4f}")
        print(f"peak_stress_scaler factor: {peak_stress_scaler['factor']:.2f}")
        print(f"peak_strain_scaler factor: {peak_strain_scaler['factor']:.4f}")
        
        # Normalize XGB_fc and CatBoost_peak_strain
        X_XGB_fc_normalized = X_XGB_fc_original / peak_stress_scaler['factor']
        X_CatBoost_strain_normalized = X_CatBoost_strain_original / peak_strain_scaler['factor']
        
        print(f"\nNormalized XGB_fc range: {np.min(X_XGB_fc_normalized):.6f} - {np.max(X_XGB_fc_normalized):.6f}")
        print(f"Normalized XGB_fc mean: {np.mean(X_XGB_fc_normalized):.6f}")
        print(f"Normalized CatBoost_peak_strain range: {np.min(X_CatBoost_strain_normalized):.6f} - {np.max(X_CatBoost_strain_normalized):.6f}")
        print(f"Normalized CatBoost_peak_strain mean: {np.mean(X_CatBoost_strain_normalized):.6f}")
        
        # Check if within reasonable range (should be close to 1 after peak average normalization)
        if np.abs(np.mean(X_XGB_fc_normalized) - 1.0) > 0.5:
            print(f"⚠️ Warning: Normalized XGB_fc mean ({np.mean(X_XGB_fc_normalized):.6f}) differs significantly from expected value (1.0)!")
        if np.abs(np.mean(X_CatBoost_strain_normalized) - 1.0) > 0.5:
            print(f"⚠️ Warning: Normalized CatBoost_peak_strain mean ({np.mean(X_CatBoost_strain_normalized):.6f}) differs significantly from expected value (1.0)!")
        print("==================================================\n")
    else:
        raise ValueError(f"Could not find XGB_fc or CatBoost_peak_strain columns (XGB_fc column: {xgb_fc_col}, CatBoost_peak_strain column: {catboost_strain_col})")
    
    # Data splitting - use sample_divisions to determine test set
    print("Splitting data...")
    # Test set is samples labeled 'test' in 'sample_divisions'
    test_idx = np.where(sample_divisions == 'test')[0]
    
    print(f"Data splitting results:")
    print(f"  Test set: {len(test_idx)} samples (from DataSlice labels)")
    
    # Create test set data loader - use XGB_fc and CatBoost_peak_strain as input
    print("Note: Test mode uses XGB_fc and CatBoost_peak_strain as peak inputs (instead of fc/strain_m)")
    test_dataset = ClusterAwareDataset(
        X_strain[test_idx], X_stress[test_idx], X_material[test_idx], 
        X_XGB_fc_normalized[test_idx], X_CatBoost_strain_normalized[test_idx], 
        output_length=model_info['curve_length']
    )
    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)
    
    # Perform prediction
    print("Performing test set prediction...")
    all_pred_curves = []
    all_true_curves = []
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(test_loader):
            strain = batch['strain'].to(device)
            stress = batch['stress'].to(device)
            material_params = batch['material_params'].to(device)
            peak_stress = batch['peak_stress'].to(device)  # Now XGB_fc
            peak_strain = batch['peak_strain'].to(device)  # Now CatBoost_peak_strain
            
            # Build input
            x = torch.cat([strain.unsqueeze(-1), material_params, peak_stress.unsqueeze(-1), peak_strain.unsqueeze(-1)], dim=-1)
            
            # Model prediction - add intermediate layer debugging
            if batch_idx == 0:
                # Manually track forward propagation to view intermediate outputs
                with torch.no_grad():
                    # LSTM layer
                    lstm_out, _ = model.lstm(x)
                    print(f"    LSTM output range: [{lstm_out.min().item():.6f}, {lstm_out.max().item():.6f}], mean: {lstm_out.mean().item():.6f}")
                    
                    # LayerNorm (if exists)
                    if hasattr(model, 'ln_lstm_out'):
                        lstm_out_reshaped_temp = lstm_out.reshape(-1, lstm_out.shape[-1])
                        lstm_out_normalized = model.ln_lstm_out(lstm_out_reshaped_temp)
                        lstm_out = lstm_out_normalized.reshape(lstm_out.shape)
                    
                    # FC layer - use fc_layers Sequential module
                    batch_size, seq_len, hidden_size = lstm_out.shape
                    lstm_out_reshaped = lstm_out.reshape(-1, hidden_size)
                    
                    # Check if model has fc_layers attribute
                    if not hasattr(model, 'fc_layers'):
                        raise AttributeError(
                            f"Model missing 'fc_layers' attribute. Available attributes: {[attr for attr in dir(model) if not attr.startswith('_')]}\n"
                            f"This may be because the loaded model checkpoint does not match the current model definition."
                        )
                    
                    # Pass through fc_layers Sequential module
                    curve_pred_reshaped = model.fc_layers(lstm_out_reshaped)
                    curve_pred = curve_pred_reshaped.reshape(batch_size, seq_len)
                    print(f"    FC layer output range: [{curve_pred.min().item():.6f}, {curve_pred.max().item():.6f}], mean: {curve_pred.mean().item():.6f}")
            else:
                # Normal prediction
                curve_pred = model(x)
            
            # Detailed debugging information
            if batch_idx == 0:
                print(f"\n  ========== Detailed Debugging Info - First Batch ==========")
                print(f"  Input data statistics:")
                print(f"    strain shape: {strain.shape}, range: [{strain.min().item():.6f}, {strain.max().item():.6f}], mean: {strain.mean().item():.6f}")
                print(f"    material_params shape: {material_params.shape}, range: [{material_params.min().item():.6f}, {material_params.max().item():.6f}], mean: {material_params.mean().item():.6f}")
                print(f"    peak_stress shape: {peak_stress.shape}, range: [{peak_stress.min().item():.6f}, {peak_stress.max().item():.6f}], mean: {peak_stress.mean().item():.6f}")
                print(f"    peak_strain shape: {peak_strain.shape}, range: [{peak_strain.min().item():.6f}, {peak_strain.max().item():.6f}], mean: {peak_strain.mean().item():.6f}")
                print(f"    input x shape: {x.shape}, range: [{x.min().item():.6f}, {x.max().item():.6f}], mean: {x.mean().item():.6f}")
                print(f"    input x contains NaN: {torch.isnan(x).any().item()}")
                print(f"    input x contains Inf: {torch.isinf(x).any().item()}")
                print(f"\n  Model output statistics (normalized):")
                print(f"    prediction shape: {curve_pred.shape}")
                print(f"    prediction range: [{curve_pred.min().item():.6f}, {curve_pred.max().item():.6f}]")
                print(f"    prediction mean: {curve_pred.mean().item():.6f}")
                print(f"    prediction standard deviation: {curve_pred.std().item():.6f}")
                print(f"    prediction contains NaN: {torch.isnan(curve_pred).any().item()}")
                print(f"    prediction contains Inf: {torch.isinf(curve_pred).any().item()}")
                print(f"    number of non-zero values in prediction: {(curve_pred.abs() > 1e-6).sum().item()} / {curve_pred.numel()}")
                print(f"\n  True value statistics (normalized):")
                print(f"    true range: [{stress.min().item():.6f}, {stress.max().item():.6f}]")
                print(f"    true mean: {stress.mean().item():.6f}")
                print(f"    true standard deviation: {stress.std().item():.6f}")
                print(f"    first sample prediction first 10 values: {curve_pred[0, :10].cpu().numpy()}")
                print(f"    first sample true first 10 values: {stress[0, :10].cpu().numpy()}")
                print(f"    first sample prediction peak position: {np.argmax(curve_pred[0].cpu().numpy())}")
                print(f"    first sample true peak position: {np.argmax(stress[0].cpu().numpy())}")
                print(f"    first sample prediction peak value: {curve_pred[0].max().item():.6f}")
                print(f"    first sample true peak value: {stress[0].max().item():.6f}")
                print(f"  ================================================\n")
            
            all_pred_curves.append(curve_pred.cpu().numpy())
            all_true_curves.append(stress.cpu().numpy())
    
    # Combine all prediction results
    test_pred_curves = np.concatenate(all_pred_curves, axis=0)
    test_true_curves = np.concatenate(all_true_curves, axis=0)
    
    print(f"Test set prediction completed, total {len(test_pred_curves)} samples")
    
    # Denormalize prediction results to original stress values
    print("\n========== Denormalization Process ==========")
    print(f"Before denormalization (normalized):")
    print(f"  Prediction curve range: [{test_pred_curves.min():.6f}, {test_pred_curves.max():.6f}]")
    print(f"  Prediction curve mean: {test_pred_curves.mean():.6f}")
    print(f"  Prediction curve standard deviation: {test_pred_curves.std():.6f}")
    print(f"  True curve range: [{test_true_curves.min():.6f}, {test_true_curves.max():.6f}]")
    print(f"  True curve mean: {test_true_curves.mean():.6f}")
    
    if stress_scaler and stress_scaler['type'] == 'peak_average':
        print(f"\nUsing peak stress average {stress_scaler['factor']:.2f} for denormalization")
        test_pred_curves_original = test_pred_curves * stress_scaler['factor']
        test_true_curves_original = test_true_curves * stress_scaler['factor']
        print(f"After denormalization (original values):")
        print(f"  Prediction curve range: [{test_pred_curves_original.min():.2f}, {test_pred_curves_original.max():.2f}]")
        print(f"  Prediction curve mean: {test_pred_curves_original.mean():.2f}")
        print(f"  True curve range: [{test_true_curves_original.min():.2f}, {test_true_curves_original.max():.2f}]")
        print(f"  True curve mean: {test_true_curves_original.mean():.2f}")
    else:
        test_pred_curves_original = test_pred_curves
        test_true_curves_original = test_true_curves
        print("Stress data is already in original values, using directly...")
    print("=====================================\n")
    
    # Calculate test set metrics
    test_metrics = calculate_curve_metrics(test_pred_curves_original, test_true_curves_original)
    
    # Define metric names
    test_metric_names = {
        'Curve_R2': ('Curve R²', 'Curve R²'),
        'Curve_MSE': ('Curve MSE', 'Curve MSE'), 
        'Mean_DTW_Distance': ('DTW Distance', 'DTW Distance'),
        'Peak_R2': ('Peak R²', 'Peak R²'),
        'Mean_Shape_Similarity': ('Shape Similarity', 'Shape Similarity'),
        'Mean_Shape_Aware_Score': ('Shape Aware Score', 'Shape Aware Score'),
        'Mean_Monotonicity_Score': ('Monotonicity Score', 'Monotonicity Score'),
        'Mean_Curvature_Score': ('Curvature Score', 'Curvature Score')
    }
    
    # Detailed analysis
    print(f"\n=== Test Set Performance Analysis ===")
    print(f"Test set sample count: {len(test_idx)}")
    
    # Peak stress and strain check
    print(f"\n=== Peak Stress and Strain Check ===")
    print(f"Test set peak stress range: {np.min(X_peak_stress[test_idx]):.2f} - {np.max(X_peak_stress[test_idx]):.2f}")
    print(f"Test set peak strain range: {np.min(X_peak_strain[test_idx]):.4f} - {np.max(X_peak_strain[test_idx]):.4f}")
    
    # Prediction result analysis
    pred_peaks = np.max(test_pred_curves_original, axis=1)
    true_peaks = np.max(test_true_curves_original, axis=1)
    print(f"\n=== Prediction Result Analysis ===")
    print(f"Predicted peak range: {np.min(pred_peaks):.2f} - {np.max(pred_peaks):.2f}")
    print(f"True peak range: {np.min(true_peaks):.2f} - {np.max(true_peaks):.2f}")
    print(f"Predicted peak standard deviation: {np.std(pred_peaks):.2f}")
    print(f"True peak standard deviation: {np.std(true_peaks):.2f}")
    
    # Check prediction quality
    if np.std(pred_peaks) < 1.0:
        print("⚠️ Warning: Predicted peaks show very little variation, model may output constant values")
    else:
        print("✅ Predicted peaks show normal variation")
    
    # Second prediction: use fc/strain_m as input to generate Curve 2
    print(f"\nPerforming second prediction (using fc/strain_m as peak inputs)...")
    test_dataset_fc = ClusterAwareDataset(
        X_strain[test_idx], X_stress[test_idx], X_material[test_idx], 
        X_peak_stress[test_idx], X_peak_strain[test_idx],  # Use fc/strain_m
        output_length=model_info['curve_length']
    )
    test_loader_fc = DataLoader(test_dataset_fc, batch_size=16, shuffle=False)
    
    all_pred_curves_fc = []
    with torch.no_grad():
        for batch in test_loader_fc:
            strain = batch['strain'].to(device)
            material_params = batch['material_params'].to(device)
            peak_stress = batch['peak_stress'].to(device)  # fc/strain_m
            peak_strain = batch['peak_strain'].to(device)  # fc/strain_m
            
            x = torch.cat([strain.unsqueeze(-1), material_params, peak_stress.unsqueeze(-1), peak_strain.unsqueeze(-1)], dim=-1)
            curve_pred = model(x)
            all_pred_curves_fc.append(curve_pred.cpu().numpy())
    
    test_pred_curves_fc = np.concatenate(all_pred_curves_fc, axis=0)
    test_pred_curves_fc_original = test_pred_curves_fc * stress_scaler['factor']
    
    print(f"Second prediction completed, total {len(test_pred_curves_fc_original)} samples")
    
    # Calculate metrics for Curve 2 and Curve 3 separately
    print(f"\n=== Curve 2 Metrics (fc/strain_m input) ===")
    test_metrics_fc = calculate_curve_metrics(test_pred_curves_fc_original, test_true_curves_original)
    print("+" + "-" * 35 + "+" + "-" * 20 + "+")
    print(f"|{'Metric Name':<35}|{'Curve 2 Result':<20}|")
    print("+" + "-" * 35 + "+" + "-" * 20 + "+")
    for metric_key, metric_name_tup in test_metric_names.items():
        english_name, _ = metric_name_tup
        display_name = f"{english_name}"
        test_val = test_metrics_fc[metric_key]
        if abs(test_val) < 0.001:
            val_str = f"{test_val:.8f}"
        elif abs(test_val) < 1:
            val_str = f"{test_val:.6f}"
        else:
            val_str = f"{test_val:.4f}"
        print(f"|{display_name:<35}|{val_str:<20}|")
    print("+" + "-" * 35 + "+" + "-" * 20 + "+")
    
    print(f"\n=== Curve 3 Metrics (XGB_fc/CatBoost_peak_strain input) ===")
    print("+" + "-" * 35 + "+" + "-" * 20 + "+")
    print(f"|{'Metric Name':<35}|{'Curve 3 Result':<20}|")
    print("+" + "-" * 35 + "+" + "-" * 20 + "+")
    for metric_key, metric_name_tup in test_metric_names.items():
        english_name, _ = metric_name_tup
        display_name = f"{english_name}"
        test_val = test_metrics[metric_key]
        if abs(test_val) < 0.001:
            val_str = f"{test_val:.8f}"
        elif abs(test_val) < 1:
            val_str = f"{test_val:.6f}"
        else:
            val_str = f"{test_val:.4f}"
        print(f"|{display_name:<35}|{val_str:<20}|")
    print("+" + "-" * 35 + "+" + "-" * 20 + "+")
    
    # Get test labels
    test_labels_legacy = sample_divisions[test_idx] if sample_divisions is not None else None
    
    # Generate test set prediction comparison plot - show all test samples (three curves: true, fc/strain_m, predicted)
    print(f"\nGenerating test set prediction comparison plot (test mode: three curves)...")
    plot_test_predictions_three_curves(
        pred_curves=np.array(test_pred_curves_original),  # Curve 3
        true_curves=np.array(test_true_curves_original),  # Curve 1
        fc_curves=np.array(test_pred_curves_fc_original),  # Curve 2: fc/strain_m prediction
        test_material_params=X_material[test_idx],
        strain_data=X_strain[test_idx],  # Use normalized strain data
        save_dir=save_dir,
        num_plots=None,  # Show all test samples
        strain_scaler=strain_scaler,
        stress_scaler=stress_scaler,
        test_labels=test_labels_legacy
    )
    
    # Save test results (exclude non-serializable objects)
    model_info_dict = {
        'model_params': model_info['model_params'],
        'best_params': model_info['best_params'],
        'material_param_names': model_info['material_param_names'],
        'curve_length': model_info['curve_length'],
        'strain_scaler': model_info['strain_scaler'],
        'stress_scaler': model_info['stress_scaler'],
        'peak_stress_scaler': model_info['peak_stress_scaler'],
        'peak_strain_scaler': model_info['peak_strain_scaler']
        # Exclude material_scaler (MinMaxScaler object)
    }
    
    # Optional fields
    if 'epoch' in model_info:
        model_info_dict['epoch'] = model_info['epoch']
    if 'loss' in model_info:
        model_info_dict['loss'] = model_info['loss']
    if 'best_fold_info' in model_info:
        model_info_dict['best_fold_info'] = model_info['best_fold_info']
    
    test_results = {
        'test_metrics_curve3': test_metrics,  # Curve 3 metrics
        'test_metrics_curve2': test_metrics_fc,  # Curve 2 metrics
        'test_pred_curves_curve3': test_pred_curves_original.tolist(),  # Curve 3 predictions
        'test_pred_curves_curve2': test_pred_curves_fc_original.tolist(),  # Curve 2 predictions: fc/strain_m
        'test_true_curves': test_true_curves_original.tolist(),
        'test_indices': test_idx.tolist(),
        'model_info': model_info_dict,
        'prediction_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    
    with open(os.path.join(save_dir, 'test_results.json'), 'w') as f:
        json.dump(test_results, f, indent=2)
    
    print(f"\nTest results saved to: {save_dir}")
    
    return test_results


def predict_test_set(model, model_info, excel_file, save_dir='SAVE/test_predictions',
                     use_surrogate_peak=False, surrogate_cols=('XGB_fc', 'CatBoost_peak_strain')):
    """
    Evaluate test set using real peaks consistent with training phase, optionally compare with surrogate peak inputs.
    """
    print("=== Test Set Prediction (Real Peak Baseline) ===")
    os.makedirs(save_dir, exist_ok=True)
    
    print("Loading data...")
    X_strain, X_stress, X_material, X_peak_stress, X_peak_strain, material_param_names, strain_scaler, stress_scaler, material_scaler, peak_stress_scaler, peak_strain_scaler, X_material_original, X_stress_original, X_peak_stress_original, X_peak_strain_original, sample_divisions = load_excel_data(
        excel_file, excel_file, model_info['curve_length'])
    print(f"Loaded {len(X_strain)} samples")
    
    test_mask = np.array([str(lbl).lower().startswith('test') for lbl in sample_divisions])
    test_idx = np.where(test_mask)[0]
    if len(test_idx) == 0:
        raise ValueError("No test/test* labels found in DataSlice, cannot build test set.")
    
    unique_labels, counts = np.unique(sample_divisions[test_idx], return_counts=True)
    print("Test set label statistics:")
    for lbl, cnt in zip(unique_labels, counts):
        print(f"  {lbl}: {cnt}")
    print(f"  Total: {len(test_idx)}")

    # Try to load predicted peak data (for prediction mode)
    surrogate_data = None
    try:
        surrogate_data = _load_surrogate_peak_columns(excel_file, peak_stress_scaler, peak_strain_scaler, surrogate_cols)
        print(f"\nSuccessfully read predicted peak columns: stress='{surrogate_data['stress_col']}', strain='{surrogate_data['strain_col']}'")
        print(f"Original peak stress range: {np.min(surrogate_data['stress_original']):.2f} - {np.max(surrogate_data['stress_original']):.2f}, mean: {np.mean(surrogate_data['stress_original']):.2f}")
        print(f"Original peak strain range: {np.min(surrogate_data['strain_original']):.4f} - {np.max(surrogate_data['strain_original']):.4f}, mean: {np.mean(surrogate_data['strain_original']):.4f}")
        print(f"Normalized peak stress range: {np.min(surrogate_data['stress_normalized']):.6f} - {np.max(surrogate_data['stress_normalized']):.6f}")
        print(f"Normalized peak strain range: {np.min(surrogate_data['strain_normalized']):.6f} - {np.max(surrogate_data['strain_normalized']):.6f}")
    except ValueError as exc:
        print(f"⚠️ Warning: {exc}")
        print("Will skip prediction mode, only run real value input mode.")
        surrogate_data = None
    
    # Attention weight save flag
    attn_saved = {'RealPeak-Model': False, 'PredictedPeak-Model': False}
    
    def _postprocess_initial_segment(pred_curves, strain_curves, initial_ratio=0.15):
        """
        Postprocess initial segment to ensure elastic behavior (start from 0, monotonically increasing, smooth slope)
        
        Args:
            pred_curves: predicted stress curves [n_samples, curve_length]
            strain_curves: strain curves [n_samples, curve_length]
            initial_ratio: ratio of initial segment (e.g., 0.15 means first 15% of points)
        
        Returns:
            corrected_curves: corrected prediction curves
        """
        corrected_curves = pred_curves.copy()
        n_samples, curve_length = pred_curves.shape
        
        for i in range(n_samples):
            pred_curve = corrected_curves[i]
            strain_curve = strain_curves[i]
            
            # Determine index range for initial segment
            initial_length = max(3, int(curve_length * initial_ratio))
            
            # Find peak position (to determine reasonable range for initial segment)
            peak_idx = np.argmax(pred_curve)
            if peak_idx < initial_length:
                initial_length = min(initial_length, max(3, peak_idx // 2))
            
            # Ensure first point is 0
            pred_curve[0] = 0.0
            
            # If second point is less than 0, set to 0 as well
            if pred_curve[1] < 0:
                pred_curve[1] = 0.0
            
            # Calculate average slope of initial segment (use middle part to avoid first point anomalies)
            if initial_length >= 3:
                # Use points 2 to initial_length to calculate average slope
                valid_indices = np.arange(2, min(initial_length, len(pred_curve)))
                if len(valid_indices) > 0:
                    strain_diff = strain_curve[valid_indices] - strain_curve[valid_indices[0]-1]
                    stress_diff = pred_curve[valid_indices] - pred_curve[valid_indices[0]-1]
                    valid_mask = strain_diff > 1e-8
                    if valid_mask.sum() > 0:
                        avg_slope = np.mean(stress_diff[valid_mask] / strain_diff[valid_mask])
                    else:
                        avg_slope = (pred_curve[initial_length-1] - pred_curve[0]) / (strain_curve[initial_length-1] - strain_curve[0] + 1e-8)
                else:
                    avg_slope = (pred_curve[initial_length-1] - pred_curve[0]) / (strain_curve[initial_length-1] - strain_curve[0] + 1e-8)
            else:
                avg_slope = (pred_curve[min(1, initial_length-1)] - pred_curve[0]) / (strain_curve[min(1, initial_length-1)] - strain_curve[0] + 1e-8)
            
            # Ensure initial segment is monotonically increasing with smooth slope
            for j in range(1, initial_length):
                # Ensure monotonic increase
                if pred_curve[j] < pred_curve[j-1]:
                    pred_curve[j] = pred_curve[j-1]
                
                # Calculate current slope
                if strain_curve[j] > strain_curve[j-1] + 1e-8:
                    curr_slope = (pred_curve[j] - pred_curve[j-1]) / (strain_curve[j] - strain_curve[j-1])
                    
                    # If slope is abnormal (too large or too small), recalculate using average slope
                    if j >= 2:
                        if curr_slope > avg_slope * 5 or curr_slope < avg_slope * 0.2:
                            # Correct using average slope
                            pred_curve[j] = pred_curve[j-1] + avg_slope * (strain_curve[j] - strain_curve[j-1])
                        elif curr_slope < 0:
                            # If slope is negative, use previous value
                            pred_curve[j] = pred_curve[j-1]
                    else:
                        # For first two points, use average slope
                        if curr_slope < 0 or curr_slope > avg_slope * 10:
                            pred_curve[j] = pred_curve[j-1] + avg_slope * (strain_curve[j] - strain_curve[j-1])
            
            # Ensure corrected curve is still monotonically increasing (second check)
            for j in range(1, initial_length):
                if pred_curve[j] < pred_curve[j-1]:
                    pred_curve[j] = pred_curve[j-1]
        
        return corrected_curves
    
    def _predict_with_peaks(peak_stress_arr, peak_strain_arr, tag, verbose=False):
        dataset = ClusterAwareDataset(
            X_strain[test_idx], X_stress[test_idx], X_material[test_idx],
            peak_stress_arr, peak_strain_arr,
            output_length=model_info['curve_length']
        )
        loader = DataLoader(dataset, batch_size=16, shuffle=False)
        preds, trues = [], []
        strain_data_list = []
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(loader):
                strain = batch['strain'].to(device)
                stress = batch['stress'].to(device)
                material_params = batch['material_params'].to(device)
                peak_stress = batch['peak_stress'].to(device)
                peak_strain = batch['peak_strain'].to(device)
                
                x = torch.cat([strain.unsqueeze(-1), material_params,
                               peak_stress.unsqueeze(-1), peak_strain.unsqueeze(-1)], dim=-1)
                curve_pred = model(x)
                
                # Save attention visualization for first batch
                if batch_idx == 0 and tag in attn_saved and not attn_saved[tag]:
                    attn = getattr(model, 'last_attn_weights', None)
                    save_attention_visual(attn, save_dir, prefix=tag)
                    attn_saved[tag] = True
                
                if verbose and batch_idx == 0:
                    print(f"\n[{tag}] First batch debugging info")
                    print(f"  Input range: [{x.min().item():.6f}, {x.max().item():.6f}], mean: {x.mean().item():.6f}")
                    print(f"  Prediction range: [{curve_pred.min().item():.6f}, {curve_pred.max().item():.6f}], mean: {curve_pred.mean().item():.6f}")
                    print(f"  True range: [{stress.min().item():.6f}, {stress.max().item():.6f}]")
                
                preds.append(curve_pred.cpu().numpy())
                trues.append(stress.cpu().numpy())
                strain_data_list.append(strain.cpu().numpy())
        
        preds = np.concatenate(preds, axis=0)
        trues = np.concatenate(trues, axis=0)
        strain_data = np.concatenate(strain_data_list, axis=0)
        
        # Postprocess initial segment to ensure elastic behavior
        print(f"[{tag}] Postprocessing initial segment to ensure elastic behavior...")
        preds = _postprocess_initial_segment(preds, strain_data, initial_ratio=0.15)
        
        print(f"[{tag}] Prediction completed, total {len(preds)} samples")
        return preds, trues
    
    def _denormalize(curves):
        if stress_scaler and stress_scaler.get('type') == 'peak_average':
            return curves * stress_scaler['factor']
        return curves

    def _build_formula_curves(peak_stress_subset, peak_strain_subset, tag):
        if peak_stress_subset is None or peak_strain_subset is None:
            return None, None
        xiao_curves, yan_curves = [], []
        peak_stress_subset = np.asarray(peak_stress_subset, dtype=float)
        peak_strain_subset = np.asarray(peak_strain_subset, dtype=float)
        for offset, sample_idx in enumerate(test_idx):
            sigma_norm = peak_stress_subset[offset]
            epsilon_norm = peak_strain_subset[offset]
            sigma_cp = sigma_norm * peak_stress_scaler['factor'] if peak_stress_scaler and peak_stress_scaler.get('type') == 'peak_average' else sigma_norm
            epsilon_cp = epsilon_norm * peak_strain_scaler['factor'] if peak_strain_scaler and peak_strain_scaler.get('type') == 'peak_average' else epsilon_norm
            strain_seq = X_strain[sample_idx]
            if strain_scaler and strain_scaler.get('type') == 'peak_average':
                strain_seq = strain_seq * strain_scaler['factor']
            else:
                strain_seq = strain_seq.copy()
            if strain_seq.size == 0:
                strain_seq = np.zeros(model_info['curve_length'], dtype=float)
            r_value = None
            wa_value = None
            try:
                r_value = float(X_material_original[sample_idx, 6])
            except Exception:
                r_value = 0.0
            try:
                wa_value = float(X_material_original[sample_idx, 7])
            except Exception:
                wa_value = 0.0
            xiao_curves.append(calculate_xiao_curve_with_real_peaks(sigma_cp, epsilon_cp, r_value, strain_seq))
            yan_curves.append(calculate_yan_curve_with_real_peaks(sigma_cp, epsilon_cp, wa_value, strain_seq))
        print(f"[{tag}] Generated Xiao/Yan baseline curves completed, total {len(xiao_curves)} curves")
        return np.array(xiao_curves), np.array(yan_curves)
    
    def _save_metrics_to_excel(metrics_dict, save_dir, mode_name):
        """
        Save metrics to Excel file
        
        Args:
            metrics_dict: dictionary containing all method metrics
            save_dir: save directory
            mode_name: mode name
        """
        os.makedirs(save_dir, exist_ok=True)
        
        # Define metric name mapping
        metric_names = {
            'Curve_R2': 'Curve R²',
            'Curve_RMSE': 'Curve RMSE',
            'Curve_MAE': 'Curve MAE',
            'Mean_DTW_Distance': 'DTW Distance',
            'Peak_R2': 'Peak R²',
            'Peak_RMSE': 'Peak RMSE',
            'Mean_Shape_Similarity': 'Shape Similarity',
            'Mean_Shape_Aware_Score': 'Shape Aware Score',
            'Mean_Monotonicity_Score': 'Monotonicity Score',
            'Mean_Curvature_Score': 'Curvature Score'
        }
        
        # Prepare data
        data = []
        for method_name, metrics in metrics_dict.items():
            if not metrics:  # Skip empty dictionaries
                continue
            row = {'Method': method_name}
            for metric_key, metric_english_name in metric_names.items():
                if metric_key in metrics:
                    row[metric_english_name] = metrics[metric_key]
                else:
                    row[metric_english_name] = None
            data.append(row)
        
        # Create DataFrame
        df = pd.DataFrame(data)
        
        # Save to Excel
        safe_mode_name = mode_name.replace(' ', '_').replace('/', '_')
        excel_path = os.path.join(save_dir, f'metrics_comparison_{safe_mode_name}.xlsx')
        df.to_excel(excel_path, index=False, engine='openpyxl')
        print(f"\nMetrics saved to: {excel_path}")
        return excel_path
    
    def _save_comprehensive_comparison_excel(real_mode_metrics, predicted_mode_metrics, save_dir, 
                                             energy_analysis=None):
        """
        Save comprehensive comparison Excel file for two modes, including energy analysis
        
        Args:
            real_mode_metrics: metrics dictionary for real value input mode
            predicted_mode_metrics: metrics dictionary for prediction mode
            save_dir: save directory
            energy_analysis: energy analysis data dictionary containing energies and metrics
        """
        os.makedirs(save_dir, exist_ok=True)
        
        # Define metric name mapping
        metric_names = {
            'Curve_R2': 'Curve R²',
            'Curve_RMSE': 'Curve RMSE',
            'Curve_MAE': 'Curve MAE',
            'Mean_DTW_Distance': 'DTW Distance',
            'Peak_R2': 'Peak R²',
            'Peak_RMSE': 'Peak RMSE',
            'Mean_Shape_Similarity': 'Shape Similarity',
            'Mean_Shape_Aware_Score': 'Shape Aware Score',
            'Mean_Monotonicity_Score': 'Monotonicity Score',
            'Mean_Curvature_Score': 'Curvature Score'
        }
        
        # Prepare data
        data = []
        
        # Add real value input mode data
        for method_name, metrics in real_mode_metrics.items():
            if not metrics:  # Skip empty dictionaries
                continue
            row = {'Mode': 'Real Value Input Mode', 'Method': method_name}
            for metric_key, metric_english_name in metric_names.items():
                if metric_key in metrics:
                    row[metric_english_name] = metrics[metric_key]
                else:
                    row[metric_english_name] = None
            data.append(row)
        
        # Add prediction mode data
        for method_name, metrics in predicted_mode_metrics.items():
            if not metrics:  # Skip empty dictionaries
                continue
            row = {'Mode': 'Prediction Mode', 'Method': method_name}
            for metric_key, metric_english_name in metric_names.items():
                if metric_key in metrics:
                    row[metric_english_name] = metrics[metric_key]
                else:
                    row[metric_english_name] = None
            data.append(row)
        
        # Create DataFrame
        df = pd.DataFrame(data)
        
        # Rearrange column order, put mode and method first
        cols = ['Mode', 'Method'] + [metric_english_name for metric_english_name in metric_names.values()]
        df = df[[col for col in cols if col in df.columns]]
        
        # Save to Excel (use ExcelWriter to support multiple sheets)
        excel_path = os.path.join(save_dir, 'metrics_comparison_comprehensive.xlsx')
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='Metrics Comparison', index=False)
            
            # If there is energy analysis data, add energy analysis sheet
            if energy_analysis:
                energy_energies = energy_analysis.get('energies', {})
                energy_metrics = energy_analysis.get('metrics', {})
                energy_energies_20 = energy_analysis.get('energies_20%', {})
                energy_metrics_20 = energy_analysis.get('metrics_20%', {})
                
                # ========== Energy analysis for entire curve ==========
                if energy_energies:
                    # 1) Energy statistics table for entire curve
                    energy_stats_rows = []
                    for name, values in energy_energies.items():
                        values_arr = np.asarray(values, dtype=float)
                        if values_arr.size == 0:
                            continue
                        energy_stats_rows.append({
                            'Curve': name,
                            'Average Energy (MPa)': np.mean(values_arr),
                            'Standard Deviation (MPa)': np.std(values_arr),
                            'Minimum Value (MPa)': np.min(values_arr),
                            'Maximum Value (MPa)': np.max(values_arr),
                        })
                    if energy_stats_rows:
                        energy_stats_df = pd.DataFrame(energy_stats_rows)
                        energy_stats_df.to_excel(writer, sheet_name='Energy Statistics (Entire Curve)', index=False)
                    
                    # 2) Energy metric comparison table for entire curve
                    energy_metric_names = {
                        "Energy_R2": "Energy R²",
                        "Energy_MSE": "Energy MSE",
                        "Energy_RMSE": "Energy RMSE",
                        "Energy_MAE": "Energy MAE",
                        "Energy_MAPE": "Energy MAPE",
                        "Energy_Bias": "Energy Bias",
                        "Energy_Mean_Ratio": "Average Energy Ratio"
                    }
                    metric_rows = []
                    for metric_key, metric_display in energy_metric_names.items():
                        row = {'Metric Name': metric_display}
                        for method_name, metric_dict in energy_metrics.items():
                            value = metric_dict.get(metric_key)
                            if value is None:
                                row[method_name] = ""
                            elif metric_key == "Energy_MAPE":
                                row[method_name] = f"{value * 100:.4f}%"
                            else:
                                row[method_name] = value
                        metric_rows.append(row)
                    if metric_rows:
                        energy_metric_df = pd.DataFrame(metric_rows)
                        energy_metric_df.to_excel(writer, sheet_name='Energy Metrics (Entire Curve)', index=False)
                    
                    # 3) Energy detail table for entire curve (per sample)
                    true_values = np.asarray(energy_energies.get('Curve 1 (True)', []), dtype=float)
                    if true_values.size > 0:
                        energy_detail_rows = []
                        n_samples = true_values.size
                        for sample_idx in range(n_samples):
                            row = {
                                'Sample_ID': sample_idx + 1,
                                'Energy_True (MPa)': true_values[sample_idx],
                            }
                            for method_name in energy_metrics.keys():
                                energies = np.asarray(energy_energies.get(method_name, []), dtype=float)
                                if energies.size == n_samples:
                                    value = energies[sample_idx]
                                    row[f'{method_name}'] = value
                                    row[f'{method_name}_Error'] = value - true_values[sample_idx]
                            energy_detail_rows.append(row)
                        if energy_detail_rows:
                            energy_detail_df = pd.DataFrame(energy_detail_rows)
                            energy_detail_df.to_excel(writer, sheet_name='Energy Details (Entire Curve)', index=False)
                
                # ========== Energy analysis for 20% residual strength ==========
                if energy_energies_20:
                    # 1) Energy statistics table for 20% residual strength
                    energy_stats_rows_20 = []
                    for name, values in energy_energies_20.items():
                        values_arr = np.asarray(values, dtype=float)
                        if values_arr.size == 0:
                            continue
                        energy_stats_rows_20.append({
                            'Curve': name,
                            'Average Energy (MPa)': np.mean(values_arr),
                            'Standard Deviation (MPa)': np.std(values_arr),
                            'Minimum Value (MPa)': np.min(values_arr),
                            'Maximum Value (MPa)': np.max(values_arr),
                        })
                    if energy_stats_rows_20:
                        energy_stats_df_20 = pd.DataFrame(energy_stats_rows_20)
                        energy_stats_df_20.to_excel(writer, sheet_name='Energy Statistics (20% Residual Strength)', index=False)
                    
                    # 2) Energy metric comparison table for 20% residual strength
                    energy_metric_names = {
                        "Energy_R2": "Energy R²",
                        "Energy_MSE": "Energy MSE",
                        "Energy_RMSE": "Energy RMSE",
                        "Energy_MAE": "Energy MAE",
                        "Energy_MAPE": "Energy MAPE",
                        "Energy_Bias": "Energy Bias",
                        "Energy_Mean_Ratio": "Average Energy Ratio"
                    }
                    metric_rows_20 = []
                    for metric_key, metric_display in energy_metric_names.items():
                        row = {'Metric Name': metric_display}
                        for method_name, metric_dict in energy_metrics_20.items():
                            value = metric_dict.get(metric_key)
                            if value is None:
                                row[method_name] = ""
                            elif metric_key == "Energy_MAPE":
                                row[method_name] = f"{value * 100:.4f}%"
                            else:
                                row[method_name] = value
                        metric_rows_20.append(row)
                    if metric_rows_20:
                        energy_metric_df_20 = pd.DataFrame(metric_rows_20)
                        energy_metric_df_20.to_excel(writer, sheet_name='Energy Metrics (20% Residual Strength)', index=False)
                    
                    # 3) Energy detail table for 20% residual strength (per sample)
                    true_values_20 = np.asarray(energy_energies_20.get('Curve 1 (True) [20%]', []), dtype=float)
                    if true_values_20.size > 0:
                        energy_detail_rows_20 = []
                        n_samples = true_values_20.size
                        for sample_idx in range(n_samples):
                            row = {
                                'Sample_ID': sample_idx + 1,
                                'Energy_True (MPa)': true_values_20[sample_idx],
                            }
                            for method_name in energy_metrics_20.keys():
                                # Find corresponding energy values (method name may not contain [20%] suffix)
                                method_key = None
                                for key in energy_energies_20.keys():
                                    if method_name in key:
                                        method_key = key
                                        break
                                if method_key:
                                    energies = np.asarray(energy_energies_20.get(method_key, []), dtype=float)
                                    if energies.size == n_samples:
                                        value = energies[sample_idx]
                                        row[f'{method_name}'] = value
                                        row[f'{method_name}_Error'] = value - true_values_20[sample_idx]
                            energy_detail_rows_20.append(row)
                        if energy_detail_rows_20:
                            energy_detail_df_20 = pd.DataFrame(energy_detail_rows_20)
                            energy_detail_df_20.to_excel(writer, sheet_name='Energy Details (20% Residual Strength)', index=False)
        
        print(f"\nComprehensive comparison metrics saved to: {excel_path}")
        return excel_path
    
    def _export_curves_to_excel(true_curves, real_pred_curves, xiao_real_curves, yan_real_curves,
                                surrogate_pred_curves, xiao_surrogate_curves, yan_surrogate_curves,
                                strain_data, strain_scaler, save_dir, test_labels=None):
        """
        Export all mode curve data to Excel file
        Each sample contains 7 curves: True, Model_RealPeak, Xiao_RealPeak, Yan_RealPeak, 
                            Model_SurrogatePeak, Xiao_SurrogatePeak, Yan_SurrogatePeak
        
        Args:
            true_curves: true curves [n_samples, curve_length]
            real_pred_curves: model prediction curves for real value input mode [n_samples, curve_length]
            xiao_real_curves: Xiao formula curves for real value input mode [n_samples, curve_length] or None
            yan_real_curves: Yan formula curves for real value input mode [n_samples, curve_length] or None
            surrogate_pred_curves: model prediction curves for prediction mode [n_samples, curve_length] or None
            xiao_surrogate_curves: Xiao formula curves for prediction mode [n_samples, curve_length] or None
            yan_surrogate_curves: Yan formula curves for prediction mode [n_samples, curve_length] or None
            strain_data: strain data [n_samples, curve_length] (normalized)
            strain_scaler: strain normalizer
            save_dir: save directory
            test_labels: test set label array for worksheet naming [n_samples] or None
        """
        print("\n========== Exporting Curve Data to Excel File ==========")
        excel_path = os.path.join(save_dir, 'curves_data_export.xlsx')
        
        # Denormalize strain data
        if strain_scaler and strain_scaler.get('type') == 'peak_average':
            strain_data_original = strain_data * strain_scaler['factor']
        else:
            strain_data_original = strain_data.copy()
        
        n_samples = len(true_curves)
        curve_length = true_curves.shape[1]
        
        # Check if there is prediction mode data
        has_surrogate = surrogate_pred_curves is not None
        
        # Function to clean worksheet names (Excel worksheet name limitations: cannot contain : \ / ? * [ ], max 31 characters)
        def clean_sheet_name(name, max_length=31):
            """Clean worksheet name by removing Excel-disallowed characters"""
            if name is None:
                return None
            # Convert to string
            name_str = str(name)
            # Remove Excel-disallowed characters
            invalid_chars = [':', '\\', '/', '?', '*', '[', ']']
            for char in invalid_chars:
                name_str = name_str.replace(char, '_')
            # Limit length
            if len(name_str) > max_length:
                name_str = name_str[:max_length]
            return name_str
        
        # Generate worksheet name list
        sheet_names = []
        for sample_idx in range(n_samples):
            if test_labels is not None and sample_idx < len(test_labels):
                label = clean_sheet_name(test_labels[sample_idx])
                if label:
                    sheet_names.append(label)
                else:
                    sheet_names.append(f'Sample_{sample_idx+1}')
            else:
                sheet_names.append(f'Sample_{sample_idx+1}')
        
        # Check for duplicate worksheet names, add sequence numbers if needed
        seen = {}
        for i, name in enumerate(sheet_names):
            if name in seen:
                seen[name] += 1
                sheet_names[i] = f"{name}_{seen[name]}"
            else:
                seen[name] = 0
        
        # Create Excel writer
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            # Sheet 1: Combined data for all samples (contains 7 curves)
            print(f"Creating combined data table for all samples (total {n_samples} samples, 7 curves)...")
            all_samples_data = []
            for sample_idx in range(n_samples):
                # Use test label as Sample_ID (if available)
                sample_id = sheet_names[sample_idx] if test_labels is not None else sample_idx + 1
                
                for point_idx in range(curve_length):
                    row = {
                        'Sample_ID': sample_id,
                        'Point_Index': point_idx + 1,
                        'Strain': strain_data_original[sample_idx, point_idx],
                        'True': true_curves[sample_idx, point_idx],
                        'Model_RealPeak': real_pred_curves[sample_idx, point_idx],
                    }
                    # Add formula curves for real value input mode
                    if xiao_real_curves is not None:
                        row['Xiao_RealPeak'] = xiao_real_curves[sample_idx, point_idx]
                    if yan_real_curves is not None:
                        row['Yan_RealPeak'] = yan_real_curves[sample_idx, point_idx]
                    
                    # Add curves for prediction mode (if available)
                    if has_surrogate:
                        row['Model_SurrogatePeak'] = surrogate_pred_curves[sample_idx, point_idx]
                        if xiao_surrogate_curves is not None:
                            row['Xiao_SurrogatePeak'] = xiao_surrogate_curves[sample_idx, point_idx]
                        if yan_surrogate_curves is not None:
                            row['Yan_SurrogatePeak'] = yan_surrogate_curves[sample_idx, point_idx]
                    
                    all_samples_data.append(row)
            
            all_samples_df = pd.DataFrame(all_samples_data)
            all_samples_df.to_excel(writer, sheet_name='Combined Data for All Samples', index=False)
            
            # Sheet 2-N+1: Individual worksheet for each sample (contains 7 curves)
            print(f"Creating individual worksheets for each sample (total {n_samples} samples)...")
            for sample_idx in range(n_samples):
                sheet_name = sheet_names[sample_idx]
                sample_data = {
                    'Strain': strain_data_original[sample_idx],
                    'True': true_curves[sample_idx],
                    'Model_RealPeak': real_pred_curves[sample_idx],
                }
                
                # Add formula curves for real value input mode
                if xiao_real_curves is not None:
                    sample_data['Xiao_RealPeak'] = xiao_real_curves[sample_idx]
                if yan_real_curves is not None:
                    sample_data['Yan_RealPeak'] = yan_real_curves[sample_idx]
                
                # Add curves for prediction mode (if available)
                if has_surrogate:
                    sample_data['Model_SurrogatePeak'] = surrogate_pred_curves[sample_idx]
                    if xiao_surrogate_curves is not None:
                        sample_data['Xiao_SurrogatePeak'] = xiao_surrogate_curves[sample_idx]
                    if yan_surrogate_curves is not None:
                        sample_data['Yan_SurrogatePeak'] = yan_surrogate_curves[sample_idx]
                
                sample_df = pd.DataFrame(sample_data)
                sample_df.to_excel(writer, sheet_name=sheet_name, index=False)
        
        print(f"Curve data saved to: {excel_path}")
        print(f"  - Combined Data for All Samples: contains 7 curves data for all {n_samples} samples")
        if test_labels is not None:
            print(f"  - Worksheet Names: {', '.join(sheet_names[:5])}{'...' if len(sheet_names) > 5 else ''}")
        else:
            print(f"  - Sample_1 to Sample_{n_samples}: detailed 7 curves data for each sample")
        print(f"  Total worksheets: {n_samples + 1}")
        print("========================================\n")
        return excel_path
    
    # ========== Mode 1: Real Value Input Mode ==========
    print("\n" + "="*100)
    print("="*100)
    print("Mode 1: Real Value Input Mode")
    print("="*100)
    print("Using fc and peak_strain (real peaks) as inputs")
    
    # Model prediction (using real peaks)
    real_pred_norm, real_true_norm = _predict_with_peaks(
        X_peak_stress[test_idx], X_peak_strain[test_idx], tag="RealPeak-Model", verbose=True)
    real_pred_curves = _denormalize(real_pred_norm)
    real_true_curves = _denormalize(real_true_norm)
    real_metrics = calculate_curve_metrics(real_pred_curves, real_true_curves)
    
    # Xiao and Yan formulas (using real peaks)
    xiao_real_curves, yan_real_curves = _build_formula_curves(X_peak_stress[test_idx], X_peak_strain[test_idx], tag="RealPeak-Formula")
    xiao_real_metrics = calculate_curve_metrics(xiao_real_curves, real_true_curves) if xiao_real_curves is not None else None
    yan_real_metrics = calculate_curve_metrics(yan_real_curves, real_true_curves) if yan_real_curves is not None else None
    
    # Print metrics for real value mode
    print("\n=== Real Value Input Mode - Model Prediction Metrics ===")
    print(f"Curve_R2: {real_metrics['Curve_R2']:.6f}")
    print(f"Curve_RMSE: {real_metrics['Curve_RMSE']:.6f}")
    print(f"Mean_DTW_Distance: {real_metrics['Mean_DTW_Distance']:.6f}")
    print(f"Peak_R2: {real_metrics['Peak_R2']:.6f}")
    
    if xiao_real_metrics is not None:
        print("\n=== Real Value Input Mode - Xiao Formula Metrics ===")
        print(f"Curve_R2: {xiao_real_metrics['Curve_R2']:.6f}")
        print(f"Curve_RMSE: {xiao_real_metrics['Curve_RMSE']:.6f}")
        print(f"Mean_DTW_Distance: {xiao_real_metrics['Mean_DTW_Distance']:.6f}")
        print(f"Peak_R2: {xiao_real_metrics['Peak_R2']:.6f}")
    
    if yan_real_metrics is not None:
        print("\n=== Real Value Input Mode - Yan Formula Metrics ===")
        print(f"Curve_R2: {yan_real_metrics['Curve_R2']:.6f}")
        print(f"Curve_RMSE: {yan_real_metrics['Curve_RMSE']:.6f}")
        print(f"Mean_DTW_Distance: {yan_real_metrics['Mean_DTW_Distance']:.6f}")
        print(f"Peak_R2: {yan_real_metrics['Peak_R2']:.6f}")
    
    # Plot real value mode graphs
    print("\nPlotting comparison graphs for real value input mode...")
    plot_test_predictions_four_curves(
        true_curves=real_true_curves,
        model_curves=real_pred_curves,
        xiao_curves=xiao_real_curves,
        yan_curves=yan_real_curves,
        test_material_params=X_material[test_idx],
        strain_data=X_strain[test_idx],
        save_dir=save_dir,
        mode_name='Real Value Input Mode',
        num_plots=None,
        strain_scaler=strain_scaler,
        stress_scaler=stress_scaler,
        test_labels=sample_divisions[test_idx] if sample_divisions is not None else None
    )
    
    # Save metrics for real value mode to Excel
    real_mode_metrics = {
        'Model Prediction': real_metrics,
        'Xiao Formula': xiao_real_metrics if xiao_real_metrics is not None else {},
        'Yan Formula': yan_real_metrics if yan_real_metrics is not None else {}
    }
    _save_metrics_to_excel(real_mode_metrics, save_dir, 'Real Value Input Mode')
    
    # ========== Mode 2: Prediction Mode ==========
    print("\n" + "="*100)
    print("="*100)
    print("Mode 2: Prediction Mode")
    print("="*100)
    print("Using XGB_fc and CatBoost_peak_strain (predicted peaks) as inputs")
    
    # Initialize prediction mode variables (ensure always defined in function scope)
    surrogate_pred_curves = None
    surrogate_metrics = None
    xiao_surrogate_curves = None
    yan_surrogate_curves = None
    xiao_surrogate_metrics = None
    yan_surrogate_metrics = None
    predicted_mode_metrics = {}
    
    # Check if predicted peak data is available
    if surrogate_data is None:
        print("⚠️ Warning: Could not find predicted peak data, skipping prediction mode")
        print("Please ensure Excel file contains XGB_fc and CatBoost_peak_strain columns")
    else:
        # Model prediction (using predicted peaks)
        surrogate_stress_norm = surrogate_data['stress_normalized'][test_idx]
        surrogate_strain_norm = surrogate_data['strain_normalized'][test_idx]
        surrogate_pred_norm, _ = _predict_with_peaks(
            surrogate_stress_norm, surrogate_strain_norm,
            tag="PredictedPeak-Model", verbose=True)
        surrogate_pred_curves = _denormalize(surrogate_pred_norm)
        surrogate_metrics = calculate_curve_metrics(surrogate_pred_curves, real_true_curves)
        
        # Xiao and Yan formulas (using predicted peaks)
        xiao_surrogate_curves, yan_surrogate_curves = _build_formula_curves(surrogate_stress_norm, surrogate_strain_norm, tag="PredictedPeak-Formula")
        xiao_surrogate_metrics = calculate_curve_metrics(xiao_surrogate_curves, real_true_curves) if xiao_surrogate_curves is not None else None
        yan_surrogate_metrics = calculate_curve_metrics(yan_surrogate_curves, real_true_curves) if yan_surrogate_curves is not None else None
       
        # Print metrics for prediction mode
        print("\n=== 预测模式 - 模型预测指标 ===")
        print(f"Curve_R2: {surrogate_metrics['Curve_R2']:.6f}")
        print(f"Curve_RMSE: {surrogate_metrics['Curve_RMSE']:.6f}")
        print(f"Mean_DTW_Distance: {surrogate_metrics['Mean_DTW_Distance']:.6f}")
        print(f"Peak_R2: {surrogate_metrics['Peak_R2']:.6f}")
        
        if xiao_surrogate_metrics is not None:
            print("\n=== 预测模式 - Xiao公式指标 ===")
            print(f"Curve_R2: {xiao_surrogate_metrics['Curve_R2']:.6f}")
            print(f"Curve_RMSE: {xiao_surrogate_metrics['Curve_RMSE']:.6f}")
            print(f"Mean_DTW_Distance: {xiao_surrogate_metrics['Mean_DTW_Distance']:.6f}")
            print(f"Peak_R2: {xiao_surrogate_metrics['Peak_R2']:.6f}")
        
        if yan_surrogate_metrics is not None:
            print("\n=== 预测模式 - Yan公式指标 ===")
            print(f"Curve_R2: {yan_surrogate_metrics['Curve_R2']:.6f}")
            print(f"Curve_RMSE: {yan_surrogate_metrics['Curve_RMSE']:.6f}")
            print(f"Mean_DTW_Distance: {yan_surrogate_metrics['Mean_DTW_Distance']:.6f}")
            print(f"Peak_R2: {yan_surrogate_metrics['Peak_R2']:.6f}")
        
         # Plot prediction mode figures
        print("\nPlotting prediction mode comparison figures...")
        plot_test_predictions_four_curves(
            true_curves=real_true_curves,
            model_curves=surrogate_pred_curves,
            xiao_curves=xiao_surrogate_curves,
            yan_curves=yan_surrogate_curves,
            test_material_params=X_material[test_idx],
            strain_data=X_strain[test_idx],
            save_dir=save_dir,
            mode_name='Prediction Mode',
            num_plots=None,
            strain_scaler=strain_scaler,
            stress_scaler=stress_scaler,
            test_labels=sample_divisions[test_idx] if sample_divisions is not None else None
        )
        
        # Save prediction mode metrics to Excel
        predicted_mode_metrics = {
            'Model Prediction': surrogate_metrics,
            'Xiao Formula': xiao_surrogate_metrics if xiao_surrogate_metrics is not None else {},
            'Yan Formula': yan_surrogate_metrics if yan_surrogate_metrics is not None else {}
        }
        _save_metrics_to_excel(predicted_mode_metrics, save_dir, 'Prediction Mode')
    
    # ========== Energy Analysis ==========
    print("\n" + "="*100)
    print("="*100)
    print("Energy Analysis (Energy Absorbed per Unit Volume, MPa ≈ MJ/m³)")
    print("="*100)
    print("Note: Calculating two types of energy metrics simultaneously:")
    print("  1. Energy of the entire curve (energy definition used in training)")
    print("  2. Energy corresponding to 20% residual peak stress (civil engineering standard)")
    print("="*100)
    
    # Denormalize strain data (for energy integration)
    if strain_scaler and strain_scaler.get('type') == 'peak_average':
        strain_sequences_original = X_strain[test_idx] * strain_scaler['factor']
    else:
        strain_sequences_original = X_strain[test_idx].copy()
    
    num_test_samples = len(test_idx)
    
    # Get peak stresses (for 20% residual strength calculation)
    if stress_scaler and stress_scaler.get('type') == 'peak_average':
        peak_stresses_original = X_peak_stress_original[test_idx]
    else:
        peak_stresses_original = np.max(real_true_curves, axis=1)
    
    # ========== Calculate energy of entire curve (definition used in training) ==========
    print("\n--- Energy Metrics of Entire Curve (Definition Used in Training) ---")
    energy_true = np.array(
        [compute_curve_energy(strain_sequences_original[i], real_true_curves[i]) 
         for i in range(num_test_samples)], dtype=float
    )
    energy_model_real = np.array(
        [compute_curve_energy(strain_sequences_original[i], real_pred_curves[i]) 
         for i in range(num_test_samples)], dtype=float
    )
    energy_xiao_real = np.array(
        [compute_curve_energy(strain_sequences_original[i], xiao_real_curves[i]) 
         for i in range(num_test_samples)], dtype=float
    ) if xiao_real_curves is not None else np.array([])
    energy_yan_real = np.array(
        [compute_curve_energy(strain_sequences_original[i], yan_real_curves[i]) 
         for i in range(num_test_samples)], dtype=float
    ) if yan_real_curves is not None else np.array([])
    
    energy_sets = {
        "Curve 1 (True)": energy_true,
        "Model Prediction (Real Peak)": energy_model_real,
        "Xiao Formula (Real Peak)": energy_xiao_real,
        "Yan Formula (Real Peak)": energy_yan_real,
    }
    
    # Calculate energy for prediction mode if available
    if surrogate_pred_curves is not None:
        energy_model_surrogate = np.array(
            [compute_curve_energy(strain_sequences_original[i], surrogate_pred_curves[i]) 
             for i in range(num_test_samples)], dtype=float
        )
        energy_sets["Model Prediction (Surrogate Peak)"] = energy_model_surrogate
    
    if xiao_surrogate_curves is not None:
        energy_xiao_surrogate = np.array(
            [compute_curve_energy(strain_sequences_original[i], xiao_surrogate_curves[i]) 
             for i in range(num_test_samples)], dtype=float
        )
        energy_sets["Xiao Formula (Surrogate Peak)"] = energy_xiao_surrogate
    
    if yan_surrogate_curves is not None:
        energy_yan_surrogate = np.array(
            [compute_curve_energy(strain_sequences_original[i], yan_surrogate_curves[i]) 
             for i in range(num_test_samples)], dtype=float
        )
        energy_sets["Yan Formula (Surrogate Peak)"] = energy_yan_surrogate
    
    # ========== Calculate energy for 20% residual strength (civil engineering standard) ==========
    print("\n--- Energy Metrics for 20% Residual Peak Stress (Civil Engineering Standard) ---")
    energy_true_20 = np.array(
        [compute_curve_energy(strain_sequences_original[i], real_true_curves[i], 
                              residual_ratio=0.2, peak_stress=peak_stresses_original[i]) 
         for i in range(num_test_samples)], dtype=float
    )
    energy_model_real_20 = np.array(
        [compute_curve_energy(strain_sequences_original[i], real_pred_curves[i], 
                              residual_ratio=0.2, peak_stress=peak_stresses_original[i]) 
         for i in range(num_test_samples)], dtype=float
    )
    energy_xiao_real_20 = np.array(
        [compute_curve_energy(strain_sequences_original[i], xiao_real_curves[i], 
                              residual_ratio=0.2, peak_stress=peak_stresses_original[i]) 
         for i in range(num_test_samples)], dtype=float
    ) if xiao_real_curves is not None else np.array([])
    energy_yan_real_20 = np.array(
        [compute_curve_energy(strain_sequences_original[i], yan_real_curves[i], 
                              residual_ratio=0.2, peak_stress=peak_stresses_original[i]) 
         for i in range(num_test_samples)], dtype=float
    ) if yan_real_curves is not None else np.array([])
    
    energy_sets_20 = {
        "Curve 1 (True) [20%]": energy_true_20,
        "Model Prediction (Real Peak) [20%]": energy_model_real_20,
        "Xiao Formula (Real Peak) [20%]": energy_xiao_real_20,
        "Yan Formula (Real Peak) [20%]": energy_yan_real_20,
    }
    
    # Calculate 20% residual strength energy for prediction mode if available
    if surrogate_pred_curves is not None:
        energy_model_surrogate_20 = np.array(
            [compute_curve_energy(strain_sequences_original[i], surrogate_pred_curves[i], 
                                  residual_ratio=0.2, peak_stress=peak_stresses_original[i]) 
             for i in range(num_test_samples)], dtype=float
        )
        energy_sets_20["Model Prediction (Surrogate Peak) [20%]"] = energy_model_surrogate_20
    
    if xiao_surrogate_curves is not None:
        energy_xiao_surrogate_20 = np.array(
            [compute_curve_energy(strain_sequences_original[i], xiao_surrogate_curves[i], 
                                  residual_ratio=0.2, peak_stress=peak_stresses_original[i]) 
             for i in range(num_test_samples)], dtype=float
        )
        energy_sets_20["Xiao Formula (Surrogate Peak) [20%]"] = energy_xiao_surrogate_20
    
    if yan_surrogate_curves is not None:
        energy_yan_surrogate_20 = np.array(
            [compute_curve_energy(strain_sequences_original[i], yan_surrogate_curves[i], 
                                  residual_ratio=0.2, peak_stress=peak_stresses_original[i]) 
             for i in range(num_test_samples)], dtype=float
        )
        energy_sets_20["Yan Formula (Surrogate Peak) [20%]"] = energy_yan_surrogate_20
    
    # Print energy statistics for entire curve
    print("\n【Energy Statistics for Entire Curve】")
    header = f"{'Curve':<40}{'Mean Energy':>15}{'Std':>15}{'Min':>15}{'Max':>15}"
    print(header)
    print("-" * len(header))
    for name, values in energy_sets.items():
        if values.size == 0:
            print(f"{name:<40}{'—':>15}{'—':>15}{'—':>15}{'—':>15}")
            continue
        print(f"{name:<40}{np.mean(values):>15.4f}{np.std(values):>15.4f}{np.min(values):>15.4f}{np.max(values):>15.4f}")
    
    # Print energy statistics for 20% residual strength
    print("\n【Energy Statistics for 20% Residual Peak Stress】")
    header_20 = f"{'Curve':<40}{'Mean Energy':>15}{'Std':>15}{'Min':>15}{'Max':>15}"
    print(header_20)
    print("-" * len(header_20))
    for name, values in energy_sets_20.items():
        if values.size == 0:
            print(f"{name:<40}{'—':>15}{'—':>15}{'—':>15}{'—':>15}")
            continue
        print(f"{name:<40}{np.mean(values):>15.4f}{np.std(values):>15.4f}{np.min(values):>15.4f}{np.max(values):>15.4f}")
    
    # Calculate energy metrics for entire curve
    energy_metrics_dict = {}
    energy_metrics_dict['Model Prediction (Real Peak)'] = compute_energy_metrics(energy_true, energy_model_real)
    if energy_xiao_real.size > 0:
        energy_metrics_dict['Xiao Formula (Real Peak)'] = compute_energy_metrics(energy_true, energy_xiao_real)
    if energy_yan_real.size > 0:
        energy_metrics_dict['Yan Formula (Real Peak)'] = compute_energy_metrics(energy_true, energy_yan_real)
    
    if surrogate_pred_curves is not None:
        energy_metrics_dict['Model Prediction (Surrogate Peak)'] = compute_energy_metrics(energy_true, energy_model_surrogate)
    if xiao_surrogate_curves is not None:
        energy_metrics_dict['Xiao Formula (Surrogate Peak)'] = compute_energy_metrics(energy_true, energy_xiao_surrogate)
    if yan_surrogate_curves is not None:
        energy_metrics_dict['Yan Formula (Surrogate Peak)'] = compute_energy_metrics(energy_true, energy_yan_surrogate)
    
    # Calculate energy metrics for 20% residual strength
    energy_metrics_dict_20 = {}
    energy_metrics_dict_20['Model Prediction (Real Peak)'] = compute_energy_metrics(energy_true_20, energy_model_real_20)
    if energy_xiao_real_20.size > 0:
        energy_metrics_dict_20['Xiao Formula (Real Peak)'] = compute_energy_metrics(energy_true_20, energy_xiao_real_20)
    if energy_yan_real_20.size > 0:
        energy_metrics_dict_20['Yan Formula (Real Peak)'] = compute_energy_metrics(energy_true_20, energy_yan_real_20)
    
    if surrogate_pred_curves is not None:
        energy_metrics_dict_20['Model Prediction (Surrogate Peak)'] = compute_energy_metrics(energy_true_20, energy_model_surrogate_20)
    if xiao_surrogate_curves is not None:
        energy_metrics_dict_20['Xiao Formula (Surrogate Peak)'] = compute_energy_metrics(energy_true_20, energy_xiao_surrogate_20)
    if yan_surrogate_curves is not None:
        energy_metrics_dict_20['Yan Formula (Surrogate Peak)'] = compute_energy_metrics(energy_true_20, energy_yan_surrogate_20)
    
    # Print energy prediction metrics for entire curve
    print(f"\n=== Energy Prediction Metrics for Entire Curve (Compared with True Curve Energy) ===")
    metric_display_order = ["Energy_R2", "Energy_MSE", "Energy_RMSE", "Energy_MAE", "Energy_MAPE", "Energy_Bias", "Energy_Mean_Ratio"]
    metric_display_names = {
        "Energy_R2": "Energy R²",
        "Energy_MSE": "Energy MSE",
        "Energy_RMSE": "Energy RMSE",
        "Energy_MAE": "Energy MAE",
        "Energy_MAPE": "Energy MAPE",
        "Energy_Bias": "Energy Bias",
        "Energy_Mean_Ratio": "Mean Energy Ratio"
    }
    
    row_header = f"{'Metric Name':<20}"
    for name in energy_metrics_dict.keys():
        row_header += f"{name:>25}"
    print(row_header)
    print("-" * len(row_header))
    for metric_key in metric_display_order:
        row = f"{metric_display_names[metric_key]:<20}"
        for _, metric_dict in energy_metrics_dict.items():
            value = metric_dict.get(metric_key)
            if value is None:
                row += f"{'—':>25}"
            elif metric_key == "Energy_MAPE":
                row += f"{value * 100:>24.2f}%"
            else:
                row += f"{value:>25.6f}"
        print(row)
    
    # Print energy prediction metrics for 20% residual strength
    print(f"\n=== Energy Prediction Metrics for 20% Residual Peak Stress (Compared with True Curve Energy) ===")
    row_header_20 = f"{'Metric Name':<20}"
    for name in energy_metrics_dict_20.keys():
        row_header_20 += f"{name:>25}"
    print(row_header_20)
    print("-" * len(row_header_20))
    for metric_key in metric_display_order:
        row = f"{metric_display_names[metric_key]:<20}"
        for _, metric_dict in energy_metrics_dict_20.items():
            value = metric_dict.get(metric_key)
            if value is None:
                row += f"{'—':>25}"
            elif metric_key == "Energy_MAPE":
                row += f"{value * 100:>24.2f}%"
            else:
                row += f"{value:>25.6f}"
        print(row)
    print("="*100)
    
    # Prepare energy analysis data (including both energy metrics)
    energy_analysis_data = {
        'energies': energy_sets,  # Energy of entire curve
        'energies_20%': energy_sets_20,  # Energy for 20% residual strength
        'metrics': energy_metrics_dict,  # Energy metrics for entire curve
        'metrics_20%': energy_metrics_dict_20  # Energy metrics for 20% residual strength
    }
    
    # Save comprehensive comparison Excel file (including all metrics and energy analysis for both modes)
    if predicted_mode_metrics:
        _save_comprehensive_comparison_excel(real_mode_metrics, predicted_mode_metrics, save_dir, 
                                             energy_analysis=energy_analysis_data)
    
    # Print comprehensive comparison table
    print("\n" + "="*100)
    print("="*100)
    print("Comprehensive Metrics Comparison Table - Performance Comparison of All Methods")
    print("="*100)
    
    # Define metric name mapping
    metric_names = {
        'Curve_R2': ('Curve R²', 'Curve R²'),
        'Curve_RMSE': ('Curve RMSE', 'Curve RMSE'),
        'Curve_MAE': ('Curve MAE', 'Curve MAE'),
        'Mean_DTW_Distance': ('DTW Distance', 'DTW Distance'),
        'Peak_R2': ('Peak R²', 'Peak R²'),
        'Peak_RMSE': ('Peak RMSE', 'Peak RMSE'),
        'Mean_Shape_Similarity': ('Shape Similarity', 'Shape Similarity'),
        'Mean_Shape_Aware_Score': ('Shape Aware Score', 'Shape Aware Score'),
        'Mean_Monotonicity_Score': ('Monotonicity Score', 'Monotonicity Score')
    }
    
    # Collect metrics for all methods
    all_methods_metrics = []
    
    # 1. Model Prediction (Real Peak)
    all_methods_metrics.append(('Model Prediction (Real Peak)', real_metrics))
    
    # 2. Xiao Formula (Real Peak)
    if xiao_real_metrics is not None:
        all_methods_metrics.append(('Xiao Formula (Real Peak)', xiao_real_metrics))
    
    # 3. Yan Formula (Real Peak)
    if yan_real_metrics is not None:
        all_methods_metrics.append(('Yan Formula (Real Peak)', yan_real_metrics))
    
    # 4. Model Prediction (Surrogate Peak)
    if surrogate_metrics is not None:
        all_methods_metrics.append(('Model Prediction (Surrogate Peak)', surrogate_metrics))
    
    # 5. Xiao Formula (Surrogate Peak)
    if xiao_surrogate_metrics is not None:
        all_methods_metrics.append(('Xiao Formula (Surrogate Peak)', xiao_surrogate_metrics))
    
    # 6. Yan Formula (Surrogate Peak)
    if yan_surrogate_metrics is not None:
        all_methods_metrics.append(('Yan Formula (Surrogate Peak)', yan_surrogate_metrics))
    
    # Print table
    # Header
    header = f"{'Metric':<25}"
    for method_name, _ in all_methods_metrics:
        header += f"{method_name:<30}"
    print(header)
    print("-" * len(header))
    
    # Print each metric
    for metric_key, (english_name, english_label) in metric_names.items():
        row = f"{english_name} ({english_label}):".ljust(25)
        for method_name, metrics_dict in all_methods_metrics:
            if metric_key in metrics_dict:
                val = metrics_dict[metric_key]
                if abs(val) < 0.001:
                    val_str = f"{val:.8f}"
                elif abs(val) < 1:
                    val_str = f"{val:.6f}"
                else:
                    val_str = f"{val:.4f}"
                row += f"{val_str:<30}"
            else:
                row += f"{'N/A':<30}"
        print(row)
    
    print("="*100)
    print("="*100)
    
    # Find best method for each metric
    print("\nBest Method for Each Metric:")
    print("-" * 80)
    for metric_key, (english_name, english_label) in metric_names.items():
        best_method = None
        best_value = None
        is_higher_better = metric_key in ['Curve_R2', 'Peak_R2', 'Mean_Shape_Similarity', 
                                         'Mean_Shape_Aware_Score', 'Mean_Monotonicity_Score']
        
        for method_name, metrics_dict in all_methods_metrics:
            if metric_key in metrics_dict:
                val = metrics_dict[metric_key]
                if best_value is None:
                    best_value = val
                    best_method = method_name
                elif is_higher_better:
                    if val > best_value:
                        best_value = val
                        best_method = method_name
                else:
                    if val < best_value:
                        best_value = val
                        best_method = method_name
        
        if best_method is not None:
            if abs(best_value) < 0.001:
                val_str = f"{best_value:.8f}"
            elif abs(best_value) < 1:
                val_str = f"{best_value:.6f}"
            else:
                val_str = f"{best_value:.4f}"
            print(f"{english_name}: {best_method} ({val_str})")
    print("-" * 80)
    
    # Note: Plotting completed in respective modes, no repeated plotting here
    
    model_info_dict = {
        'model_params': model_info['model_params'],
        'best_params': model_info['best_params'],
        'material_param_names': model_info['material_param_names'],
        'curve_length': model_info['curve_length'],
        'strain_scaler': model_info['strain_scaler'],
        'stress_scaler': model_info['stress_scaler'],
        'peak_stress_scaler': model_info['peak_stress_scaler'],
        'peak_strain_scaler': model_info['peak_strain_scaler']
    }
    if 'epoch' in model_info:
        model_info_dict['epoch'] = model_info['epoch']
    if 'loss' in model_info:
        model_info_dict['loss'] = model_info['loss']
    if 'best_fold_info' in model_info:
        model_info_dict['best_fold_info'] = model_info['best_fold_info']
    
    test_results = {
        'metrics_real_peak': real_metrics,
        'test_metrics_curve2': real_metrics,  # Compatible with old field
        'pred_curves_real_peak': real_pred_curves.tolist(),
        'test_pred_curves_curve2': real_pred_curves.tolist(),
        'test_true_curves': real_true_curves.tolist(),
        'test_indices': test_idx.tolist(),
        'model_info': model_info_dict,
        'prediction_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    
    # Save Xiao and Yan formula metrics
    if xiao_real_metrics is not None:
        test_results['metrics_xiao_real_peak'] = xiao_real_metrics
        test_results['xiao_curves_real_peak'] = xiao_real_curves.tolist()
    
    if yan_real_metrics is not None:
        test_results['metrics_yan_real_peak'] = yan_real_metrics
        test_results['yan_curves_real_peak'] = yan_real_curves.tolist()
    
    if surrogate_pred_curves is not None and surrogate_metrics is not None:
        test_results['metrics_surrogate_peak'] = surrogate_metrics
        test_results['test_metrics_curve3'] = surrogate_metrics
        test_results['pred_curves_surrogate_peak'] = surrogate_pred_curves.tolist()
        test_results['test_pred_curves_curve3'] = surrogate_pred_curves.tolist()
    
    if xiao_surrogate_metrics is not None:
        test_results['metrics_xiao_surrogate_peak'] = xiao_surrogate_metrics
        test_results['xiao_curves_surrogate_peak'] = xiao_surrogate_curves.tolist()
    
    if yan_surrogate_metrics is not None:
        test_results['metrics_yan_surrogate_peak'] = yan_surrogate_metrics
        test_results['yan_curves_surrogate_peak'] = yan_surrogate_curves.tolist()
    
    # Save energy analysis data (including both energy metrics)
    test_results['energy_analysis'] = {
        'energies': {k: v.tolist() if isinstance(v, np.ndarray) else v for k, v in energy_sets.items()},
        'energies_20%': {k: v.tolist() if isinstance(v, np.ndarray) else v for k, v in energy_sets_20.items()},
        'metrics': {k: {kk: vv for kk, vv in v.items()} for k, v in energy_metrics_dict.items()},
        'metrics_20%': {k: {kk: vv for kk, vv in v.items()} for k, v in energy_metrics_dict_20.items()}
    }
    
    # Export all curve data to Excel
    _export_curves_to_excel(
        true_curves=real_true_curves,
        real_pred_curves=real_pred_curves,
        xiao_real_curves=xiao_real_curves,
        yan_real_curves=yan_real_curves,
        surrogate_pred_curves=surrogate_pred_curves,
        xiao_surrogate_curves=xiao_surrogate_curves,
        yan_surrogate_curves=yan_surrogate_curves,
        strain_data=X_strain[test_idx],
        strain_scaler=strain_scaler,
        save_dir=save_dir,
        test_labels=sample_divisions[test_idx] if sample_divisions is not None else None
    )
    
    # ========== Noise and Robustness Analysis (Curve Level) ==========
    print("\n" + "="*100)
    print("="*100)
    print("Noise and Robustness Analysis (Curve Level)")
    print("="*100)
    
    try:
        def _evaluate_with_material(material_array, tag):
            """Predict and return metrics using given normalized material parameters"""
            dataset = ClusterAwareDataset(
                X_strain[test_idx], X_stress[test_idx], material_array,
                X_peak_stress[test_idx], X_peak_strain[test_idx],
                output_length=model_info['curve_length']
            )
            loader = DataLoader(dataset, batch_size=16, shuffle=False)
            preds, trues = [], []
            with torch.no_grad():
                for batch in loader:
                    strain = batch['strain'].to(device)
                    stress = batch['stress'].to(device)
                    material_params = batch['material_params'].to(device)
                    peak_stress = batch['peak_stress'].to(device)
                    peak_strain = batch['peak_strain'].to(device)
                    x = torch.cat([strain.unsqueeze(-1), material_params,
                                   peak_stress.unsqueeze(-1), peak_strain.unsqueeze(-1)], dim=-1)
                    curve_pred = model(x) if hasattr(model, 'forward') else torch.randn_like(stress)
                    preds.append(curve_pred.cpu().numpy())
                    trues.append(stress.cpu().numpy())
            preds = np.concatenate(preds, axis=0)
            trues = np.concatenate(trues, axis=0)
            preds_denorm = preds * stress_scaler['factor'] if stress_scaler and stress_scaler.get('type') == 'peak_average' else preds
            trues_denorm = trues * stress_scaler['factor'] if stress_scaler and stress_scaler.get('type') == 'peak_average' else trues
            metrics = calculate_curve_metrics(preds_denorm, trues_denorm)
            return metrics
        
        robustness_dir = os.path.join(save_dir, 'robustness')
        os.makedirs(robustness_dir, exist_ok=True)
        print(f"Robustness analysis results will be saved to: {robustness_dir}")
        
        # 1) Noise sensitivity (add Gaussian noise to material parameters)
        noise_levels = [0.0, 0.01, 0.02, 0.05, 0.1]
        noise_records = []
        material_base = X_material[test_idx]
        mat_std = material_base.std(axis=0) + 1e-8
        print(f"Starting noise sensitivity analysis ({len(noise_levels)} noise levels)...")
        for nl in noise_levels:
            noisy_material = material_base + nl * mat_std * np.random.randn(*material_base.shape)
            metrics = _evaluate_with_material(noisy_material, tag=f"noise_{nl}")
            noise_records.append({'noise_std': nl, **metrics})
            print(f"[Noise {nl}] Curve_RMSE={metrics.get('Curve_RMSE'):.6f}, Curve_R2={metrics.get('Curve_R2'):.6f}")
        
        # 2) Outlier contamination (replace with extreme values by ratio)
        contam_levels = [0.0, 0.02, 0.05, 0.1]
        contam_records = []
        max_vals = material_base.max(axis=0)
        min_vals = material_base.min(axis=0)
        extreme_high = max_vals + 3.0 * mat_std
        extreme_low = min_vals - 3.0 * mat_std
        rng = np.random.default_rng(42)
        print(f"Starting outlier contamination analysis ({len(contam_levels)} contamination levels)...")
        for cr in contam_levels:
            polluted = material_base.copy()
            n = len(polluted)
            k = int(n * cr)
            if k > 0:
                idx_high = rng.choice(n, size=k//2 if k >= 2 else k, replace=False)
                remaining = [i for i in range(n) if i not in idx_high]
                idx_low = rng.choice(remaining, size=k - len(idx_high), replace=False) if k - len(idx_high) > 0 else []
                polluted[idx_high] = extreme_high
                if len(idx_low) > 0:
                    polluted[idx_low] = extreme_low
            metrics = _evaluate_with_material(polluted, tag=f"contam_{cr}")
            contam_records.append({'contam_ratio': cr, **metrics})
            print(f"[Contam {cr}] Curve_RMSE={metrics.get('Curve_RMSE'):.6f}, Curve_R2={metrics.get('Curve_R2'):.6f}")
        
        # 3) Bootstrap prediction intervals (based on material perturbation sampling)
        bootstrap_runs = 50
        lower_q, upper_q = 2.5, 97.5
        bootstrap_metrics = []
        rmse_list = []
        r2_list = []
        print(f"Starting Bootstrap analysis ({bootstrap_runs} runs)...")
        for b in range(bootstrap_runs):
            if (b + 1) % 10 == 0:
                print(f"  Bootstrap progress: {b+1}/{bootstrap_runs}")
            noisy = material_base + 0.02 * mat_std * np.random.randn(*material_base.shape)
            metrics = _evaluate_with_material(noisy, tag=f"bootstrap_{b}")
            bootstrap_metrics.append(metrics)
            if 'Curve_RMSE' in metrics and 'Curve_R2' in metrics:
                rmse_list.append(metrics['Curve_RMSE'])
                r2_list.append(metrics['Curve_R2'])
        rmse_arr = np.array(rmse_list)
        r2_arr = np.array(r2_list)
        bootstrap_summary = {
            'RMSE_mean': float(rmse_arr.mean()) if rmse_arr.size else None,
            'RMSE_p2.5': float(np.percentile(rmse_arr, lower_q)) if rmse_arr.size else None,
            'RMSE_p97.5': float(np.percentile(rmse_arr, upper_q)) if rmse_arr.size else None,
            'R2_mean': float(r2_arr.mean()) if r2_arr.size else None,
            'R2_p2.5': float(np.percentile(r2_arr, lower_q)) if r2_arr.size else None,
            'R2_p97.5': float(np.percentile(r2_arr, upper_q)) if r2_arr.size else None,
        }
        def _fmt_opt(val):
            return f"{val:.6f}" if val is not None else "nan"
        print(f"Bootstrap RMSE mean/p2.5/p97.5: {_fmt_opt(bootstrap_summary['RMSE_mean'])}, "
              f"{_fmt_opt(bootstrap_summary['RMSE_p2.5'])}, "
              f"{_fmt_opt(bootstrap_summary['RMSE_p97.5'])}")
        
        # 4) Simple quantile interval (residual method): pred +/- residual quantiles
        residuals = real_true_curves - real_pred_curves
        res_lower = np.percentile(residuals, lower_q, axis=0)
        res_upper = np.percentile(residuals, upper_q, axis=0)
        
        # Create DataFrames (must be outside try block for Excel saving)
        noise_df = pd.DataFrame(noise_records)
        contam_df = pd.DataFrame(contam_records)
        
        # Plot noise/contamination sensitivity curves
        try:
            plt.figure(figsize=(8, 4))
            plt.plot(noise_df['noise_std'], noise_df['Curve_RMSE'], marker='o', label='Curve RMSE', linewidth=2)
            plt.plot(noise_df['noise_std'], noise_df['Curve_R2'], marker='s', label='Curve R²', linewidth=2)
            plt.xlabel('Noise Std', fontsize=12)
            plt.ylabel('Metric Value', fontsize=12)
            plt.title('Noise Robustness Analysis', fontsize=14, fontweight='bold')
            plt.grid(True, alpha=0.3)
            plt.legend(fontsize=10)
            plt.tight_layout()
            noise_plot_path = os.path.join(robustness_dir, 'noise_sensitivity.png')
            plt.savefig(noise_plot_path, dpi=300, bbox_inches='tight')
            plt.close()
            print(f"✓ Noise sensitivity plot saved: {noise_plot_path}")
        except Exception as e_plot:
            print(f"⚠️ Failed to plot noise sensitivity curve: {e_plot}")
            import traceback
            traceback.print_exc()
        
        try:
            plt.figure(figsize=(8, 4))
            plt.plot(contam_df['contam_ratio'], contam_df['Curve_RMSE'], marker='o', label='Curve RMSE', linewidth=2)
            plt.plot(contam_df['contam_ratio'], contam_df['Curve_R2'], marker='s', label='Curve R²', linewidth=2)
            plt.xlabel('Contamination Ratio', fontsize=12)
            plt.ylabel('Metric Value', fontsize=12)
            plt.title('Outlier Robustness Analysis', fontsize=14, fontweight='bold')
            plt.grid(True, alpha=0.3)
            plt.legend(fontsize=10)
            plt.tight_layout()
            outlier_plot_path = os.path.join(robustness_dir, 'outlier_sensitivity.png')
            plt.savefig(outlier_plot_path, dpi=300, bbox_inches='tight')
            plt.close()
            print(f"✓ Outlier sensitivity plot saved: {outlier_plot_path}")
        except Exception as e_plot:
            print(f"⚠️ Failed to plot outlier sensitivity curve: {e_plot}")
            import traceback
            traceback.print_exc()
        
        # Save to Excel
        excel_path_robust = os.path.join(robustness_dir, 'robustness_analysis.xlsx')
        try:
            with pd.ExcelWriter(excel_path_robust, engine='openpyxl') as writer:
                noise_df.to_excel(writer, sheet_name='Noise_Sensitivity', index=False)
                contam_df.to_excel(writer, sheet_name='Outlier_Sensitivity', index=False)
                if bootstrap_metrics:
                    pd.DataFrame(bootstrap_metrics).to_excel(writer, sheet_name='Bootstrap_Metrics', index=False)
                pd.DataFrame([bootstrap_summary]).to_excel(writer, sheet_name='Bootstrap_Summary', index=False)
                pd.DataFrame({
                    'Residual_Lower_p2.5': res_lower,
                    'Residual_Upper_p97.5': res_upper
                }).to_excel(writer, sheet_name='Residual_Quantile_Band', index=False)
            print(f"✓ Excel file saved: {excel_path_robust}")
        except Exception as e_excel:
            print(f"⚠️ Failed to save Excel file: {e_excel}")
            import traceback
            traceback.print_exc()
        
        print(f"\nNoise/robustness analysis completed, results saved to: {robustness_dir}")
        print(f"  - Noise sensitivity plot: noise_sensitivity.png")
        print(f"  - Outlier sensitivity plot: outlier_sensitivity.png")
        print(f"  - Excel: robustness_analysis.xlsx")
    except Exception as e_robust:
        print(f"⚠️ Error during noise/robustness analysis: {e_robust}")
        import traceback
        traceback.print_exc()
        print("Continuing with subsequent code...")
    
    with open(os.path.join(save_dir, 'test_results.json'), 'w') as f:
        json.dump(test_results, f, indent=2)
    
    print(f"\nTest results saved to: {save_dir}")
    return test_results


def analyze_parameter_impact_on_energy(model, model_info, excel_file, save_dir='SAVE/energy_parameter_analysis'):
    """
    Analyze the impact of water-cement ratio (w/c) and aggregate replacement ratio (r) 
    on three energy indicators (W_u, W_p, η)
    
    Args:
        model: Trained model
        model_info: Model information dictionary
        excel_file: Excel data file path
        save_dir: Save directory
    """
    print("\n" + "="*100)
    print("="*100)
    print("Analysis of Parameter Impact on Energy Indicators")
    print("="*100)
    
    os.makedirs(save_dir, exist_ok=True)
    
    # Load data to get parameter ranges and baseline values
    print("Loading data to get parameter ranges...")
    X_strain, X_stress, X_material, X_peak_stress, X_peak_strain, material_param_names, \
    strain_scaler, stress_scaler, material_scaler, peak_stress_scaler, peak_strain_scaler, \
    X_material_original, X_stress_original, X_peak_stress_original, X_peak_strain_original, \
    sample_divisions = load_excel_data(excel_file, excel_file, model_info['curve_length'])
    
    # Get parameter indices
    # Determine indices of w/c and r based on material_param_names
    wc_idx = None
    r_idx = None
    for i, name in enumerate(material_param_names):
        name_lower = str(name).lower().strip()
        if 'w/c' in name_lower or 'water_cement' in name_lower or 'water/cement' in name_lower:
            wc_idx = i
        if name_lower == 'r' or 'replacement' in name_lower:
            r_idx = i
    
    # Use default indices if not found (based on code usage: w/c at index 2, r at index 6)
    if wc_idx is None:
        wc_idx = 2
    if r_idx is None:
        r_idx = 6
    
    print(f"Parameter indices: w/c={wc_idx}, r={r_idx}")
    
    # Get baseline parameter values (using training set average)
    train_mask = np.array([str(lbl).lower().startswith('train') for lbl in sample_divisions])
    train_idx = np.where(train_mask)[0]
    if len(train_idx) == 0:
        train_idx = np.arange(len(X_material_original))
    
    baseline_material = X_material_original[train_idx].mean(axis=0).copy()
    baseline_material_normalized = X_material[train_idx].mean(axis=0).copy()
    
    # Get w/c and r ranges
    wc_values_train = X_material_original[train_idx, wc_idx]
    r_values_train = X_material_original[train_idx, r_idx]
    
    wc_min, wc_max = wc_values_train.min(), wc_values_train.max()
    r_min, r_max = r_values_train.min(), r_values_train.max()
    
    print(f"Parameter ranges: w/c=[{wc_min:.3f}, {wc_max:.3f}], r=[{r_min:.1f}, {r_max:.1f}]")
    
    # Generate test values for w/c and r
    wc_test_values = np.linspace(wc_min, wc_max, 20)
    r_test_values = np.array([0, 25, 50, 75, 100])  # User-specified r values
    
    # Get strain sequence (normalized)
    strain_sequence_normalized = X_strain[0]  # Use strain sequence of first sample
    if strain_scaler and strain_scaler.get('type') == 'peak_average':
        strain_sequence_original = strain_sequence_normalized * strain_scaler['factor']
    else:
        strain_sequence_original = strain_sequence_normalized.copy()
    
    # Need to load XGBoost and CatBoost models for peak prediction
    print("Loading peak prediction models...")
    try:
        import xgboost as xgb
        import pickle
        
        # Try to load XGBoost and CatBoost models
        xgb_model_path = r"C:\JunzanLi_project\constitutive_relation\Pi_BiLSTM\SAVE\XGBoost\best_xgboost_model.pkl"
        catboost_model_path = r"C:\JunzanLi_project\constitutive_relation\Pi_BiLSTM\SAVE\CatBoost\best_catboost_model.pkl"
        
        xgb_model = None
        catboost_model = None
        
        if os.path.exists(xgb_model_path):
            with open(xgb_model_path, 'rb') as f:
                xgb_model = pickle.load(f)
            print(f"Successfully loaded XGBoost model: {xgb_model_path}")
        
        try:
            import catboost as cb
            if os.path.exists(catboost_model_path):
                catboost_model = cb.CatBoostRegressor()
                catboost_model.load_model(catboost_model_path)
                print(f"Successfully loaded CatBoost model: {catboost_model_path}")
        except ImportError:
            print("CatBoost not installed, will use XGBoost for peak strain prediction")
        
    except Exception as e:
        print(f"⚠️ Warning: Failed to load peak prediction models: {e}")
        print("Will use baseline peak values for prediction")
        xgb_model = None
        catboost_model = None
    
    # Calculate baseline peaks (for backup)
    baseline_peak_stress_normalized = X_peak_stress[train_idx].mean()
    baseline_peak_strain_normalized = X_peak_strain[train_idx].mean()
    
    def _predict_peaks(material_params_original, material_params_normalized):
        """Predict peak stress and strain"""
        if xgb_model is not None:
            try:
                # Predict peak stress using XGBoost
                peak_stress_pred = xgb_model.predict([material_params_original])[0]
                peak_stress_normalized = peak_stress_pred / peak_stress_scaler['factor']
            except Exception as e:
                # Use baseline peak stress
                peak_stress_normalized = baseline_peak_stress_normalized
        else:
            # Use baseline value
            peak_stress_normalized = baseline_peak_stress_normalized
        
        if catboost_model is not None:
            try:
                # Predict peak strain using CatBoost
                peak_strain_pred = catboost_model.predict([material_params_original])[0]
                peak_strain_normalized = peak_strain_pred / peak_strain_scaler['factor']
            except Exception as e:
                # Use baseline peak strain
                peak_strain_normalized = baseline_peak_strain_normalized
        else:
            # Use baseline value
            peak_strain_normalized = baseline_peak_strain_normalized
        
        return peak_stress_normalized, peak_strain_normalized
    
    def _predict_curve(material_params_normalized, peak_stress_normalized, peak_strain_normalized):
        """Predict complete curve using model"""
        model.eval()
        with torch.no_grad():
            curve_length = len(strain_sequence_normalized)
            batch_size = 1
            
            # Build input tensor, shape should be [batch_size, curve_length, feature_dim]
            # strain: [batch_size, curve_length] -> [batch_size, curve_length, 1]
            strain_tensor = torch.tensor(strain_sequence_normalized, dtype=torch.float32).unsqueeze(0).to(device)  # [1, curve_length]
            
            # material_params: [num_params] -> [batch_size, curve_length, num_params]
            material_params_tensor = torch.tensor(material_params_normalized, dtype=torch.float32).to(device)  # [num_params]
            material_tensor = material_params_tensor.unsqueeze(0).expand(curve_length, -1).unsqueeze(0)  # [1, curve_length, num_params]
            
            # peak_stress: scalar -> [batch_size, curve_length] -> [batch_size, curve_length, 1]
            peak_stress_tensor = torch.tensor([peak_stress_normalized], dtype=torch.float32).to(device)  # [1]
            peak_stress_expanded = peak_stress_tensor.expand(curve_length).unsqueeze(0).unsqueeze(-1)  # [1, curve_length, 1]
            
            # peak_strain: scalar -> [batch_size, curve_length] -> [batch_size, curve_length, 1]
            peak_strain_tensor = torch.tensor([peak_strain_normalized], dtype=torch.float32).to(device)  # [1]
            peak_strain_expanded = peak_strain_tensor.expand(curve_length).unsqueeze(0).unsqueeze(-1)  # [1, curve_length, 1]
            
            # Concatenate: [1, curve_length, 1] + [1, curve_length, num_params] + [1, curve_length, 1] + [1, curve_length, 1]
            x = torch.cat([strain_tensor.unsqueeze(-1), material_tensor, 
                          peak_stress_expanded, peak_strain_expanded], dim=-1)  # [1, curve_length, 1+num_params+1+1]
            
            stress_curve_normalized = model(x)[0].cpu().numpy() if hasattr(model, 'forward') else np.random.randn(curve_length)
            
            # Denormalize
            if stress_scaler and stress_scaler.get('type') == 'peak_average':
                stress_curve = stress_curve_normalized * stress_scaler['factor']
            else:
                stress_curve = stress_curve_normalized
            
            return stress_curve
    
    # Analysis 1: Fix r value, change w/c, analyze impact on energy indicators
    print("\nAnalysis 1: Impact of water-cement ratio (w/c) on energy indicators with fixed aggregate replacement ratio (r)...")
    results_wc = []
    
    for r_val in r_test_values:
        print(f"  Processing r={r_val}%...")
        material_base = baseline_material.copy()
        material_base_normalized = baseline_material_normalized.copy()
        
        # Set r value
        material_base[r_idx] = r_val
        if material_scaler is not None:
            # Normalize r value
            material_base_normalized[r_idx] = material_scaler.transform([material_base])[0, r_idx]
        else:
            material_base_normalized[r_idx] = r_val / 100.0 if r_val > 1 else r_val
        
        wc_results = []
        for wc_val in wc_test_values:
            # Set w/c value
            material_test = material_base.copy()
            material_test_normalized = material_base_normalized.copy()
            material_test[wc_idx] = wc_val
            if material_scaler is not None:
                material_test_normalized[wc_idx] = material_scaler.transform([material_test])[0, wc_idx]
            else:
                material_test_normalized[wc_idx] = wc_val
            
            # Predict peaks
            peak_stress_norm, peak_strain_norm = _predict_peaks(material_test, material_test_normalized)
            
            # Predict curve
            stress_curve = _predict_curve(material_test_normalized, peak_stress_norm, peak_strain_norm)
            
            # Calculate energy indicators (including both entire curve and 20% residual strength energy)
            energy_indicators = compute_energy_indicators(strain_sequence_original, stress_curve, residual_ratio=0.2)
            
            wc_results.append({
                'w/c': wc_val,
                'r': r_val,
                # Energy indicators for entire curve
                'W_u': energy_indicators['W_u'],
                'W_ascending': energy_indicators['W_ascending'],
                'W_p': energy_indicators['W_p'],
                'eta': energy_indicators['eta'],
                # Energy indicators for 20% residual strength
                'W_u_20%': energy_indicators.get('W_u_20%', 0.0),
                'W_ascending_20%': energy_indicators.get('W_ascending_20%', 0.0),
                'W_p_20%': energy_indicators.get('W_p_20%', 0.0),
                'eta_20%': energy_indicators.get('eta_20%', 0.0)
            })
        
        results_wc.extend(wc_results)
    
    # Analysis 2: Fix w/c value (use median), change r, analyze impact on energy indicators
    print("\nAnalysis 2: Impact of aggregate replacement ratio (r) on energy indicators with fixed water-cement ratio (w/c)...")
    wc_median = np.median(wc_values_train)
    print(f"  Using w/c median: {wc_median:.3f}")
    
    results_r = []
    material_base = baseline_material.copy()
    material_base_normalized = baseline_material_normalized.copy()
    material_base[wc_idx] = wc_median
    if material_scaler is not None:
        material_base_normalized[wc_idx] = material_scaler.transform([material_base])[0, wc_idx]
    else:
        material_base_normalized[wc_idx] = wc_median
    
    for r_val in r_test_values:
        # Set r value
        material_test = material_base.copy()
        material_test_normalized = material_base_normalized.copy()
        material_test[r_idx] = r_val
        if material_scaler is not None:
            material_test_normalized[r_idx] = material_scaler.transform([material_test])[0, r_idx]
        else:
            material_test_normalized[r_idx] = r_val / 100.0 if r_val > 1 else r_val
        
        # Predict peaks
        peak_stress_norm, peak_strain_norm = _predict_peaks(material_test, material_test_normalized)
        
        # Predict curve
        stress_curve = _predict_curve(material_test_normalized, peak_stress_norm, peak_strain_norm)
        
        # Calculate energy indicators (including both entire curve and 20% residual strength energy)
        energy_indicators = compute_energy_indicators(strain_sequence_original, stress_curve, residual_ratio=0.2)
        
        results_r.append({
            'w/c': wc_median,
            'r': r_val,
            # Energy indicators for entire curve
            'W_u': energy_indicators['W_u'],
            'W_ascending': energy_indicators['W_ascending'],
            'W_p': energy_indicators['W_p'],
            'eta': energy_indicators['eta'],
            # Energy indicators for 20% residual strength
            'W_u_20%': energy_indicators.get('W_u_20%', 0.0),
            'W_ascending_20%': energy_indicators.get('W_ascending_20%', 0.0),
            'W_p_20%': energy_indicators.get('W_p_20%', 0.0),
            'eta_20%': energy_indicators.get('eta_20%', 0.0)
        })
    
    # Convert to DataFrames
    df_wc = pd.DataFrame(results_wc)
    df_r = pd.DataFrame(results_r)
    
    # Plot figures (show both energy indicators)
    print("\nGenerating figures...")
    # Create 4x4 subplots: first two rows for entire curve energy, last two rows for 20% residual strength energy
    fig, axes = plt.subplots(4, 4, figsize=(24, 20))
    fig.suptitle('Analysis of Parameter Impact on Energy Indicators (Entire Curve and 20% Residual Strength Energy)', fontsize=16, fontweight='bold')
    
    # Define energy indicators
    metrics = ['W_u', 'W_ascending', 'W_p', 'eta']
    metric_names = ['Total Energy W_u (MPa)', 'Ascending Stage Energy W_ascending (MPa)', 'Descending Stage Energy W_p (MPa)', 'Energy Toughness Ratio η']
    metrics_20 = ['W_u_20%', 'W_ascending_20%', 'W_p_20%', 'eta_20%']
    metric_names_20 = ['Total Energy W_u (MPa) [20%]', 'Ascending Stage Energy W_ascending (MPa) [20%]', 'Descending Stage Energy W_p (MPa) [20%]', 'Energy Toughness Ratio η [20%]']
    
    # First row: impact of w/c on four energy indicators of entire curve (different r values)
    for col, (metric, metric_name) in enumerate(zip(metrics, metric_names)):
        ax = axes[0, col]
        for r_val in r_test_values:
            data = df_wc[df_wc['r'] == r_val]
            ax.plot(data['w/c'], data[metric], marker='o', label=f'r={r_val}%', linewidth=1.5, markersize=4)
        
        ax.set_xlabel('Water-Cement Ratio (w/c)', fontsize=11)
        ax.set_ylabel(metric_name, fontsize=11)
        ax.set_title(f'Impact of Water-Cement Ratio on {metric_name} (Entire Curve)', fontsize=12, fontweight='bold')
        ax.legend(fontsize=9)
        ax.grid(True, alpha=0.3)
    
    # Second row: impact of r on four energy indicators of entire curve
    for col, (metric, metric_name) in enumerate(zip(metrics, metric_names)):
        ax = axes[1, col]
        ax.plot(df_r['r'], df_r[metric], marker='s', color='red', linewidth=2, markersize=8)
        ax.set_xlabel('Aggregate Replacement Ratio r (%)', fontsize=11)
        ax.set_ylabel(metric_name, fontsize=11)
        ax.set_title(f'Impact of Aggregate Replacement Ratio on {metric_name} (Entire Curve)', fontsize=12, fontweight='bold')
        ax.grid(True, alpha=0.3)
        ax.set_xticks(r_test_values)
    
    # Third row: impact of w/c on four energy indicators of 20% residual strength (different r values)
for col, (metric, metric_name) in enumerate(zip(metrics_20, metric_names_20)):
    ax = axes[2, col]
    for r_val in r_test_values:
        data = df_wc[df_wc['r'] == r_val]
        ax.plot(data['w/c'], data[metric], marker='o', label=f'r={r_val}%', linewidth=1.5, markersize=4)
    
    ax.set_xlabel('Water-Cement Ratio (w/c)', fontsize=11)
    ax.set_ylabel(metric_name, fontsize=11)
    ax.set_title(f'Impact of Water-Cement Ratio on {metric_name} (20% Residual Strength)', fontsize=12, fontweight='bold')
    ax.legend(fontsize=9)
    ax.grid(True, alpha=0.3)

# Fourth row: impact of r on four energy indicators of 20% residual strength
for col, (metric, metric_name) in enumerate(zip(metrics_20, metric_names_20)):
    ax = axes[3, col]
    ax.plot(df_r['r'], df_r[metric], marker='s', color='red', linewidth=2, markersize=8)
    ax.set_xlabel('Aggregate Replacement Ratio r (%)', fontsize=11)
    ax.set_ylabel(metric_name, fontsize=11)
    ax.set_title(f'Impact of Aggregate Replacement Ratio on {metric_name} (20% Residual Strength)', fontsize=12, fontweight='bold')
    ax.grid(True, alpha=0.3)
    ax.set_xticks(r_test_values)

plt.tight_layout()
plt.savefig(os.path.join(save_dir, 'parameter_impact_on_energy.png'), dpi=300, bbox_inches='tight')
#plt.show()

# Save to Excel (including both energy indicators)
print("\nSaving results to Excel...")
excel_path = os.path.join(save_dir, 'parameter_impact_on_energy.xlsx')
with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
    # Energy indicators for entire curve
    df_wc_full = df_wc[['w/c', 'r', 'W_u', 'W_ascending', 'W_p', 'eta']]
    df_r_full = df_r[['w/c', 'r', 'W_u', 'W_ascending', 'W_p', 'eta']]
    df_wc_full.to_excel(writer, sheet_name='w_c_Impact_Analysis(Entire_Curve)', index=False)
    df_r_full.to_excel(writer, sheet_name='r_Impact_Analysis(Entire_Curve)', index=False)
    
    # Energy indicators for 20% residual strength
    df_wc_20 = df_wc[['w/c', 'r', 'W_u_20%', 'W_ascending_20%', 'W_p_20%', 'eta_20%']]
    df_r_20 = df_r[['w/c', 'r', 'W_u_20%', 'W_ascending_20%', 'W_p_20%', 'eta_20%']]
    df_wc_20.to_excel(writer, sheet_name='w_c_Impact_Analysis(20%_Residual_Strength)', index=False)
    df_r_20.to_excel(writer, sheet_name='r_Impact_Analysis(20%_Residual_Strength)', index=False)
    
    # Complete data (including all indicators)
    df_wc.to_excel(writer, sheet_name='w_c_Impact_Analysis(Complete)', index=False)
    df_r.to_excel(writer, sheet_name='r_Impact_Analysis(Complete)', index=False)

print(f"Analysis completed! Results saved to: {save_dir}")
print(f"  - Figure: {os.path.join(save_dir, 'parameter_impact_on_energy.png')}")
print(f"  - Excel: {excel_path}")

return df_wc, df_r


def main():
    """Main Function - Load Model and Perform Test Set Prediction"""
    print("=== Trained Bidirectional LSTM Model - Test Set Prediction ===")
    
    # Set file paths
    model_path = r"C:\JunzanLi_project\constitutive_relation\Pi_BiLSTM\LSTM\SAVE\bidirectional_lstm_cv\best_model.pth"
    excel_file = r"C:\JunzanLi_project\constitutive_relation\Pi_BiLSTM\dataset\dataset_final.xlsx"
    save_dir = r"C:\JunzanLi_project\constitutive_relation\Pi_BiLSTM\LSTM\SAVE\test_predictions_cv"
    use_surrogate_peak = os.environ.get('USE_SURROGATE_PEAK', '1') != '0'
    # Columns used for prediction mode: XGB_fc (peak stress) and CatBoost_peak_strain (peak strain)
    surrogate_cols = ('XGB_fc', 'CatBoost_peak_strain')
    
    # Check if files exist
    if not os.path.exists(model_path):
        print(f"Error: Model file not found {model_path}")
        print("Please run the main training script first to generate the model file")
        return None
    
    if not os.path.exists(excel_file):
        print(f"Error: Excel file not found {excel_file}")
        return None
    
    print(f"Model file: {model_path}")
    print(f"Data file: {excel_file}")
    print(f"Save directory: {save_dir}")
    
    try:
        # Load trained model
        model, model_info = load_trained_model(model_path)
        
        # Perform test set prediction
        test_results = predict_test_set(
            model, model_info, excel_file, save_dir,
            use_surrogate_peak=use_surrogate_peak,
            surrogate_cols=surrogate_cols
        )
        
        print("\n=== Prediction Completed ===")
        print("\nKey Metrics Summary:")
        print("-" * 80)
        print(f"Model Prediction (Real Peak):")
        print(f"  Curve R²: {test_results['metrics_real_peak']['Curve_R2']:.6f}")
        print(f"  Curve RMSE: {test_results['metrics_real_peak']['Curve_RMSE']:.6f}")
        print(f"  DTW Distance: {test_results['metrics_real_peak']['Mean_DTW_Distance']:.6f}")
        print(f"  Peak R²: {test_results['metrics_real_peak']['Peak_R2']:.6f}")
        
        if 'metrics_xiao_real_peak' in test_results:
            print(f"\nXiao Formula (Real Peak):")
            print(f"  Curve R²: {test_results['metrics_xiao_real_peak']['Curve_R2']:.6f}")
            print(f"  Curve RMSE: {test_results['metrics_xiao_real_peak']['Curve_RMSE']:.6f}")
            print(f"  DTW Distance: {test_results['metrics_xiao_real_peak']['Mean_DTW_Distance']:.6f}")
            print(f"  Peak R²: {test_results['metrics_xiao_real_peak']['Peak_R2']:.6f}")
        
        if 'metrics_yan_real_peak' in test_results:
            print(f"\nYan Formula (Real Peak):")
            print(f"  Curve R²: {test_results['metrics_yan_real_peak']['Curve_R2']:.6f}")
            print(f"  Curve RMSE: {test_results['metrics_yan_real_peak']['Curve_RMSE']:.6f}")
            print(f"  DTW Distance: {test_results['metrics_yan_real_peak']['Mean_DTW_Distance']:.6f}")
            print(f"  Peak R²: {test_results['metrics_yan_real_peak']['Peak_R2']:.6f}")
        
        if use_surrogate_peak and 'metrics_surrogate_peak' in test_results:
            print(f"\nModel Prediction (Surrogate Peak):")
            print(f"  Curve R²: {test_results['metrics_surrogate_peak']['Curve_R2']:.6f}")
            print(f"  Curve RMSE: {test_results['metrics_surrogate_peak']['Curve_RMSE']:.6f}")
            print(f"  DTW Distance: {test_results['metrics_surrogate_peak']['Mean_DTW_Distance']:.6f}")
            print(f"  Peak R²: {test_results['metrics_surrogate_peak']['Peak_R2']:.6f}")
        
        if 'metrics_xiao_surrogate_peak' in test_results:
            print(f"\nXiao Formula (Surrogate Peak):")
            print(f"  Curve R²: {test_results['metrics_xiao_surrogate_peak']['Curve_R2']:.6f}")
            print(f"  Curve RMSE: {test_results['metrics_xiao_surrogate_peak']['Curve_RMSE']:.6f}")
            print(f"  DTW Distance: {test_results['metrics_xiao_surrogate_peak']['Mean_DTW_Distance']:.6f}")
            print(f"  Peak R²: {test_results['metrics_xiao_surrogate_peak']['Peak_R2']:.6f}")
        
        if 'metrics_yan_surrogate_peak' in test_results:
            print(f"\nYan Formula (Surrogate Peak):")
            print(f"  Curve R²: {test_results['metrics_yan_surrogate_peak']['Curve_R2']:.6f}")
            print(f"  Curve RMSE: {test_results['metrics_yan_surrogate_peak']['Curve_RMSE']:.6f}")
            print(f"  DTW Distance: {test_results['metrics_yan_surrogate_peak']['Mean_DTW_Distance']:.6f}")
            print(f"  Peak R²: {test_results['metrics_yan_surrogate_peak']['Peak_R2']:.6f}")
        
        # Print energy analysis summary
        if 'energy_analysis' in test_results:
            print("\n" + "-" * 80)
            print("Energy Analysis Summary:")
            print("-" * 80)
            energy_analysis = test_results['energy_analysis']
            energy_metrics = energy_analysis.get('metrics', {})
            
            for method_name, metrics in energy_metrics.items():
                print(f"\n{method_name}:")
                if metrics.get('Energy_R2') is not None:
                    print(f"  Energy R²: {metrics['Energy_R2']:.6f}")
                if metrics.get('Energy_RMSE') is not None:
                    print(f"  Energy RMSE: {metrics['Energy_RMSE']:.6f}")
                if metrics.get('Energy_MAE') is not None:
                    print(f"  Energy MAE: {metrics['Energy_MAE']:.6f}")
                if metrics.get('Energy_MAPE') is not None:
                    print(f"  Energy MAPE: {metrics['Energy_MAPE'] * 100:.2f}%")
        
        print("-" * 80)
        
        return test_results
        
    except Exception as e:
        print(f"Error during prediction: {e}")
        import traceback
        traceback.print_exc()
        return None

# Script execution entry
if __name__ == "__main__":
    # Run test set prediction
    test_results = main()
